{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metagenomics-Toolkit","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Metagenomics-Toolkit allows you to run either the full pipeline of assembly, binning and many other downstream analysis tasks or individual modules. The toolkit can be configured by providing the module configuration via a yml file and a flag for the corresponding module or full pipeline mode. Options for global pipeline configuration can be viewed here.</p> <p>All tools follow the same error strategy. The execution of a tool is retried three times. If the run fails the fourth time, it will be ignored. If the execution is ignored, the toolkit will continue to run all tools that do not depend on the output of the failed tool run. Exceptions of this handling are specified in the corresponding module section.</p> <p>Note! Please do never place sensitive information in any of the yml configuration files since the configuration is part of the pipeline output.</p>"},{"location":"#run-full-pipeline","title":"Run Full Pipeline","text":"<pre><code>nextflow run main.nf -work-dir /shared/directory/test \\\n    -profile PROFILE  -resume \\\n    -entry wFullPipeline -params-file example_params/fullPipeline.yml\n</code></pre> <p>where  *  /shared/directory/test is a directory that is shared between multiple machines.  * PROFILE can be either <code>standard</code> (local use) or <code>slurm</code> depending on which environment the pipeline should be executed.</p>"},{"location":"#input","title":"Input","text":"CommandConfiguration FileTSV TableAdditional S3 Configuration <pre><code>-entry wFullPipeline -params-file example_params/fullPipeline.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>Must include the columns <code>SAMPLE</code>, <code>READS1</code> and <code>READS2</code>. <code>SAMPLE</code> must contain unique dataset identifiers without whitespaces or special characters. <code>READS1</code> and <code>READS2</code> are paired reads and can be HTTPS URLs, S3 links or files.</p> <p>Nextflow usually stores downloaded files in the work directory. If enough scratch space is available on the worker nodes then this can be prevented by specifying s3 links in the input tsv file and <code>download</code> parameter in the input yaml.</p> <p>S3 TSV Links: <pre><code>\n</code></pre></p> <p>Input YAML <pre><code>\n</code></pre></p>"},{"location":"#output-overview","title":"Output (Overview)","text":"<p>In addition to the pipeline module outputs defined in the module section (Dereplication, MagAttributes, etc), the following outputs are produced. </p> <ul> <li> <p>quality control (fastq) </p> </li> <li> <p>assembly (contigs)</p> </li> <li> <p>binning (genomes)</p> </li> <li> <p>read mapping (bam files)</p> </li> </ul>"},{"location":"#optional-run-per-sample-analysis-and-the-aggregation-of-per-sample-seperately","title":"Optional: Run per sample analysis and the aggregation of per sample seperately","text":"<p>There are two ways to execute the toolkit. You can either run all steps in one execution or you run first the per sample analysis (e.g. assembly, binning, annotation, etc.) and afterwards you combine the results (e.g. dereplication, co-occurrence) in a second run. The second option allows you to process multiple samples via independent toolkit executions on different infrastructures and combine all results afterwards.</p> <p>You would first have to run the wFullPipeline mode without dereplication, read mapping and co-occurrence modules and afterwards run the the aggregation as described below:</p>"},{"location":"#input_1","title":"Input","text":"CommandConfiguration File <pre><code>-entry wAggregatePipeline -params-file example_params/fullPipelineAggregate.yml\n</code></pre> <pre><code>\n</code></pre>"},{"location":"#elastic-metagenomic-browser-emgb","title":"Elastic Metagenomic Browser (EMGB)","text":"<p>The output generated by the Metagenomics-Toolkit can be imported into (EMGB)[https://gitlab.ub.uni-bielefeld.de/cmg/emgb/emgb-server]. EMGB allows you to easily explore your metagenomic samples in terms of MAGs, genes and their function.</p> <p>Your configuration must include at least the following analysis steps: * Assembly * Binning * Gene prediction and annotation with Prokka * Gene annotation with MMseqs * Gene taxonomy prediction with MMseqs</p> <p>We offer a script <code>bin/emgb.sh</code> that allows you to export a metagenomics toolkit output folder to emgb compatible json files.</p> <p>Example Call:</p> <pre><code>bash emgb.sh --output=output/test1 --runid=1 --binsdir=output/test1/1/binning/0.5.0/metabat --blastdb=bacmet20_predicted --name=test1\n</code></pre> <p>You can get a help page for the necessary arguments by running <code>emgb.sh --help</code>.</p>"},{"location":"#caveats","title":"Caveats","text":"<ul> <li>The pipeline breaks if <code>--stageInMode</code> is specified with <code>copy</code>.</li> </ul>"},{"location":"developer_guidelines/","title":"Guidelines","text":""},{"location":"developer_guidelines/#commit-and-release-guidelines","title":"Commit and Release Guidelines","text":"<p>We are using git-chglog to automatically generate a changelog for the latest released based on our commit messages. Commit messages should follow the following format:</p> <pre><code>feat(scope): feature added in the scope\n</code></pre> <p>Example:</p> <pre><code>feat(assembly): megahit added\n</code></pre> <p><code>feat</code> can be replaced by one of the formats specified in the options sections of the config file (see example below). Scope can for example represent a module, a configuration or a specific document.</p> <p>A new release should be made the following way: </p> <ol> <li> <p>Update pipeline version in the nextflow manifest <code>nextflow.config</code>.</p> </li> <li> <p>Create a release on Github.</p> </li> <li> <p>Run <code>git fetch</code> on the master branch to get the latest tag.</p> </li> <li> <p>Run <code>make changelog</code> and paste the output on the Github release section.</p> </li> </ol> <pre><code>\n</code></pre>"},{"location":"developer_guidelines/#versioning","title":"Versioning","text":"<p>Following semantic versioning, we define the configuration input file and the output folder structure as our public <code>API</code>. Changes to the version numbers reflect updates to the config or the output folders and files. The toolkit consists of many modules that can be used in different combinations and because of this flexibility, we had to come up with a detailed versioning system. We version each module separately, as well as the pipeline itself.</p> <p>Module MAJOR version numbers are updated when a module-specific input parameter is updated or the output folder or file structure is changed. All module version numbers can be retrieved by running the toolkit with the <code>wGetModuleVersion</code> entry point and should be reported on the release page. </p> <p>The module version number is incorporated in the output directory (see output specification)  for easier parsing of the output directory. In the following we give examples when to increment which part of the version identifier:</p> <p>Given a version number MAJOR.MINOR.PATCH, increment the:</p> <ul> <li>MAJOR version when you make incompatible changes, as for example modifying the output structure. A script that was build to parse the output structure must be adapted then.</li> <li>MINOR version when you add functionality in a backward compatible manner. One example is adding an additional tool to the module. </li> <li>PATCH version when you make backwards compatible bug fixes. This is necessary when you for example increment the docker container version number that fixes a bug or increases the     speed of the tool.</li> </ul> <p>The pipeline specific version number defined in the manifest part of the nextflow.config should be changed if either any module specific version number is incremented or any module-independent parameter (e.g. <code>tempdir</code>) or output structure is changed. </p>"},{"location":"developer_guidelines/#testing","title":"Testing","text":"<p>Tests for local use are specified in the <code>scripts</code> folder. These scripts are also used as part of the continuous integration tests. If you want to run these scripts locally, you will have to override the paths to the databases you have downloaded:</p> <p>Examples: <pre><code>bash scripts/test_fullPipeline.sh  \" --steps.magAttributes.checkm.database=/vol/spool/checkm --steps.magAttributes.gtdb.database=/vol/spool/gtdb/release202 \"\nbash scripts/test_fragmentRecruitment.sh  \" --steps.fragmentRecruitment.frhit.genomes=test/bins/small/bin.*.fa --steps.fragmentRecruitment.frhit.samples=test/reads/small/reads.tsv \"\nbash scripts/test_dereplication.sh \"  --steps.dereplication.pasolli.input=test/bins/small/attributes.tsv \"\nbash scripts/test_magAttributes.sh \"  --steps.magAttributes.input=test/bins/small/attributes.tsv \"\n</code></pre></p>"},{"location":"developer_guidelines/#nextflow-versions","title":"Nextflow Versions","text":"<p>The toolkit is tested against the lowest and highest Nextflow version number specified in VERSIONS.txt.</p>"},{"location":"developer_guidelines/#modules","title":"Modules","text":"<p>Functionality is structured in modules (assembly, binning, dereplication, .etc). Each module can have multiple workflows. Every module follows the output definition specified in the output specification  document. The name and the version of the module is specified in the <code>modules</code> section of the <code>nextflow.config</code> file.</p>"},{"location":"developer_guidelines/#workflows","title":"Workflows","text":"<ol> <li> <p>Worfklow names that can not be used directly and are just meant for internal use should start with an underscore.</p> </li> <li> <p>At least every workflow that can be used by other external workflows should contain a short description of the functionality. </p> </li> <li> <p>Workflow names must start with <code>w</code>. </p> </li> </ol>"},{"location":"developer_guidelines/#process","title":"Process","text":"<p>Process names should start <code>p</code>. The in- and output of processes should contain a sample and/or a bin and contig id. Custom error strategies that do not follow the strategy defined in nextflow.config, should be documented (see Megahit example).</p>"},{"location":"developer_guidelines/#processes-should-publish-process-specific-files","title":"Processes should publish process specific files","text":"<p>Processes should publish <code>.command.sh</code>, <code>.command.out</code>, <code>.command.log</code> and <code>.command.err</code> files but never <code>.command.run</code>. In cases where processes process different data but publish it to the same folder these files would be overwritten on every run. For example when Prokka publishes log files of every genome to the same sample directory. For that reason these files need to be renamed, so that their names include a unique id (e.g. bin id).  Please output those files to channel with the following entries and connect this channel to the pDumpLogs process that you can import from the utils module:</p> <pre><code>include { pDumpLogs } from '../utils/processes'\n\n...\n\ntuple env(FILE_ID), val(\"${output}\"), val(params.LOG_LEVELS.INFO), file(\".command.sh\"), \\\n        file(\".command.out\"), file(\".command.err\"), file(\".command.log\"), emit: logs\n</code></pre> <p>Examples can be viewed in the Checkm and Prokka process.</p>"},{"location":"developer_guidelines/#logs","title":"Logs","text":"<p>Log files should be stored in the user provided <code>logDir</code> directory.</p>"},{"location":"developer_guidelines/#log-level","title":"Log Level","text":"<p>Every configuration file must have a <code>logLevel</code> attribute that can have the following values:</p> <pre><code>ALL = 0  All logs are published\nINFO = 1 Just necessary logs are published\n</code></pre> <p>These values can be used in the publish dir directive to enable or disable the output of logs.</p> <pre><code>   publishDir params.output, mode: \"${params.publishDirMode}\", saveAs: { filename -&gt; getOutput(params.runid, \"pasolli/mash/sketch\", filename) }, \\\n        pattern: \"{**.out,**.err, **.sh, **.log}\", enabled: params.logLevel &lt;= params.LOG_LEVELS.ALL\n</code></pre> <p>Furthermore the <code>params.LOG_LEVELS.*</code> parameters can be used inside of a process to enable or disable intermediate results for debugging purposes. In cases where the log is send to the pDumpLogs process (see Process section), you can specify the log level as part of the tuple:</p> <pre><code>tuple env(FILE_ID), val(\"${output}\"), val(params.LOG_LEVELS.INFO), file(\".command.sh\"), \\\n        file(\".command.out\"), file(\".command.err\"), file(\".command.log\"), emit: logs\n</code></pre>"},{"location":"developer_guidelines/#time-limit","title":"Time Limit","text":"<p>Every process must define a time limit which will never be reached on \"normal\" execution. This limit is only useful for errors in the execution environment which could lead to an endless execution of the process.</p> <p>You can use the setTimeLimit helper method to add a user configurable time limit.</p> <p>Example:</p> <pre><code>time Utils.setTimeLimit(params.steps.qc.fastp, params.modules.qc.process.fastp.defaults, params.resources.highmemMedium)\n</code></pre>"},{"location":"developer_guidelines/#databases","title":"Databases","text":"<p>If the same database is downloaded during runtime by multiple processes, it takes up an unnecessary ammount of disc space. One idea is too always use the same place to store these databases. This place should be described in <code>params.databases</code>. If other processes try to use this databases they can look at <code>params.databases</code> on the current machine.  If it is present it can be used, if not it should be downloaded. Through this procedure only one copy of each databases is used, which is space-saving. Links to the actual database should contain the database version number or the date of download.</p>"},{"location":"developer_guidelines/#configuration","title":"Configuration","text":"<p>Every process should be configurable by providing a parameters string to the tool in the process. Every module should use the following specification in the configuration file:</p> <pre><code>steps:\n  moduleName:\n    parameter: 42\n    processName:\n      additionalParams: \" --super-flag \"\n      timeLimit: \"AUTO\"\n</code></pre> <p>Please check the process chapter regarding possible values for the time limit attribute. Additional params can have a string value (like the example above) that is provided to the tool:</p> <pre><code>pProcess {\n\n   ...\n\n  shell:\n  \"\"\"\n  supertool !{params.steps.moduleName.processName.parameter}  !{params.steps.moduleName.processName.additionalParams}\n  \"\"\"\n}\n</code></pre> <p>The value of the <code>additionalParams</code> key can also be a map if multiple tools are used in the same process:</p> <pre><code>steps:\n  moduleName:\n    parameter: 42\n    processName:\n      additionalParams:\n         toolNameA: \" -c 84  \"\n         toolNameB: \" --super-flag \"\n</code></pre> <p><code>parameter</code> fields can hold hardcoded parameters that hold a defined value like a number that should not be a string. One use case of those parameters is that they can be reused for multiple tools.</p> <p>Example:</p> <pre><code>pProcess {\n\n   ...\n\n  shell:\n  \"\"\"\n  toolNameA --super-specific-number-flag !{params.steps.moduleName.parameter}\n  toolNameB --similar-flag-to-toolA !{params.steps.moduleName.parameter} \n  \"\"\"\n}\n</code></pre>"},{"location":"developer_guidelines/#internal-configuration","title":"Internal Configuration","text":"<p>The <code>_wConfigurePipeline</code> workflow in the main.nf file should be used for setting  pipeline parameters that are need for fullfilling the user provided configuration.</p> <p>Example: Lets assume the user enables the plasmid module. In that case it is mandatory that  the assembler produces a fastg file independend of the user provided settings of the assembler. In that case the fastg parameter of any assembler will be set to <code>true</code> by the <code>_wConfigurePipeline</code> method.</p>"},{"location":"developer_guidelines/#toolkit-docker-images","title":"Toolkit Docker Images","text":"<p>Dockerfiles of Docker images that are build by toolkit developers can be found in the <code>docker</code> directory. The name of the directory (i.e.: <code>toolkit-python-env</code> in <code>docker/toolkit-python-env</code>) is used for the docker image name. All images belong to the metagenomics quay.io organisation which is owned by the Computational Metagenomics group in Bielefeld. A docker repository in the <code>metagenomics</code> orginsation must be created by the organisation owner, before the actual image can be build. The version of the image specified in the <code>VERSION</code> file (i.e. <code>docker/toolkit-python-env/VERSION</code>) is used for the image tag (<code>metagenomics/toolkit-python-env:VERSION</code>). An image build is only triggered if the version in the VERSION file is updated on the dev or master branch.</p>"},{"location":"developer_guidelines/#wiki","title":"Wiki","text":"<p>For building the documentation we are using mkdocs in combination with mkdocs-material and a plugin for building static single page html files. The wiki HTML files are uploaded to S3 storage on pull request merge events in the master and dev branch (see Makefile commands using <code>make help</code>).</p> <p>You can work on these html files locally by running <code>make dev_wiki</code>. But please note that by build the static html file for upload, the navigation might change. You can view the final html file by building the html file (see Makefile <code>make help</code>). </p>"},{"location":"developer_guidelines/#utils","title":"Utils","text":"<p>We do not want to duplicate code and thats why we should store methods in the lib/Utils.groovy file. The Utils class can be used in any module. </p>"},{"location":"developer_guidelines/#database-download","title":"Database Download","text":"<p>This section explains how a developer is able to implement the database download strategy as explained in the user documentation.  Example implementations can be found in the gtdb, checkm or rgi scripts.</p> <p>The first step is to check if the user provides an already extracted database: </p> <pre><code>DB_PATH=\"\"\nif [ -z \"!{EXTRACTED_DB}\" ]\nthen\n   # Fetch user parameter for not extracted db path and run flock (see next section)\n   DB_PATH=\"not extracted\"\nelse\n  # Set variable to extracted db path\nfi\n</code></pre> <p>Since the download is not directly handled by nextflow and paths to the files need to be downloaded, any file or directory must be mounted first to the container. For this reason you have to add the <code>setDockerMount</code> function with the database config as input to  the <code>containerOptions</code> parameter:</p> <pre><code>containerOptions \" other container options \" + setDockerMount(params.steps?.magAttributes?.checkm?.database)\n</code></pre>"},{"location":"developer_guidelines/#filesystem-locks","title":"Filesystem locks","text":"<p>Multiple jobs of the same process (e.g. GTDB) are able to synchronize the download of a database by using filesystem locks. The download is handled by the <code>concurrentDownload.sh</code> script and should be executed the following way:</p> <pre><code>flock LOCK_FILE concurrentDownload.sh --output=DATABASE \\\n           --httpsCommand=COMMAND \\\n           --localCommand=COMMAND \\\n           --s3FileCommand=COMMAND \\\n           --s3DirectoryCommand=COMMAND \\\n           --s5cmdAdditionalParams=S5CMD_PARAMS \\\n           --link=LINK \\\n           --expectedMD5SUM=USER_VERIFIED_DATABASE_MD5SUM\n</code></pre> <p>where   * <code>LOCK_FILE</code> is a file that is used for locking. Processes will check if the file is currently locked before trying to download anything.     This file should ideally placed in the <code>params.database</code> directory of the specific tool (e.g. !{params.databases}/rgi).</p> <ul> <li> <p><code>DATABASE</code> is the directory that is used for placing the specific database.</p> </li> <li> <p><code>COMMAND</code> is the command used to download and extract the database and to remove it afterwards.      (e.g. \"wget -O data.tar.gz $DOWNLOAD_LINK &amp;&amp; tar -xvf data.tar.gz ./card.json &amp;&amp; rm data.tar.gz\" for the <code>--httpsCommand</code> flag)</p> </li> <li> <p><code>USER_VERIFIED_DATABASE_MD5SUM</code> is the MD5SUM of the extracted database that the user should test manually before executing the pipeline.</p> </li> <li> <p><code>S5CMD_PARAMS</code> allows you to set s5cmd specific parameters. For more information check the s5cmd documentation. </p> </li> <li> <p><code>LINK</code> is the link that will be used to test if the file is accessible by S3, HTTPS or is available via a local path.</p> </li> <li> <p><code>USER_VERIFIED_DATABASE_MD5SUM</code> Before a database is downloaded, the script checks the MD5SUM of an already downloaded database against a user specified one.      If it does not equal, the script will download the database again.</p> </li> </ul>"},{"location":"developer_guidelines/#tests","title":"Tests","text":"<p>You can test your tool against different database inputs by using the <code>make runDatabaseTest</code> command. You will have to specify multiple databases  that are accessible via https, S3, local path etc. Please check github actions file for how to run these tests.</p>"},{"location":"developer_guidelines/#polished-variables","title":"Polished Variables","text":"<p>Sometimes user input variables must be polished before they can used in our code. Thats why the nextflow config adds a namespace to the params namespace called <code>polished</code>. For example the params.databases variable must end with a slash in order to be used as part of a docker mount. Thats why there is a variable <code>params.polished.databases</code> that should be used instead.  </p>"},{"location":"developer_guidelines/#other","title":"Other","text":"<ol> <li> <p>Magic numbers should not be used.</p> </li> <li> <p>Variable, method, workflow, folder and process names should be written in camelcase.</p> </li> </ol>"},{"location":"module_specification/","title":"Module Specification","text":""},{"location":"module_specification/#assembly","title":"Assembly","text":"<ul> <li>Version: 0.2.0</li> </ul>"},{"location":"module_specification/#output","title":"Output:","text":"<p>Assembly file names must fulfill the following name pattern:</p> <pre><code>SAMPLENAME_contigs.fa.gz\n</code></pre> <p>Contig names must be renamed according to the following pattern:</p> <p><code>SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH</code></p> <p>where</p> <ul> <li> <p><code>SAMPLEID</code> is the name of the dataset (e.g: <code>SRR234235</code>)</p> </li> <li> <p><code>SEQUENCECOUNTER</code> is the counter of the contig entry in the fasta file (e.g: 2)</p> </li> <li> <p><code>SEQUENCEHASH</code> are the last 5 characters of an md5sum hash of the fasta entry without the header and newline character.       (eg. echo -n \"ACGT\" | md5sum | cut -d ' ' -f 1 | cut -c -5 )</p> </li> </ul>"},{"location":"module_specification/#binning","title":"Binning","text":"<ul> <li>Version: 0.5.0</li> </ul>"},{"location":"module_specification/#output_1","title":"Output:","text":"<p>Binning file names must fulfill the following name pattern:</p> <pre><code>SAMPLENAME_bin.NUMBER.fa\n</code></pre> <p>Where <code>NUMBER</code> is a unique identifier per SAMPLE.</p> <p>Contig names must be renamed according to the following pattern:</p> <p><code>SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH MAG=BINNUMBER</code></p> <p>where</p> <ul> <li> <p><code>SAMPLEID</code>, <code>SEQUENCECOUNTER</code> and <code>SEQUENCEHASH</code>  definitions can be inspected in the assembly specification.</p> </li> <li> <p><code>BINNUMBER</code> is an unique identifier per SAMPLE.</p> </li> </ul>"},{"location":"module_specification/#mag-attributes","title":"MAG Attributes","text":"<ul> <li>Version: 0.1.0</li> </ul>"},{"location":"module_specification/#output_2","title":"Output:","text":"<pre><code>SAMPLENAME_TOOLNAME_CHUNK.tsv\n</code></pre> <p>where  * <code>TOOLNAME</code> could be for example <code>checkm</code>, <code>gtdb</code> etc.  * <code>CHUNK</code> is a random identifier that produces values for one part of all MAGs of a given sample.</p>"},{"location":"module_specification/#format","title":"FORMAT","text":"<p>The header line specifies the following columns: </p> <pre><code>BIN_ID  SAMPLE  BIN_ATTRIBUTE1  BIN_ATTRIBUTE2 ...\n</code></pre> <p>where    * <code>BIN_ID</code> is unique for the samples.   * <code>BIN_ATTRIBUTES</code> All column names that are not <code>BIN_ID</code> or <code>SAMPLE</code> can be any property of a MAG, like contamination, completeness etc.</p>"},{"location":"module_specification/#quality-control","title":"Quality Control","text":"<ul> <li>Version: 0.1.0</li> </ul>"},{"location":"module_specification/#output_3","title":"Output:","text":"<pre><code>SAMPLE_interleaved.qc.fq.gz\n</code></pre>"},{"location":"pipeline_configuration/","title":"Global parameter settings","text":"<ul> <li> <p><code>tempdir</code>: Temporary directory for storing files that are used to collect intermediate files.</p> </li> <li> <p><code>summary</code>: If true a summary folder is created storing results of all samples combined.</p> </li> <li> <p><code>output</code>: Output directory for storing pipeline results. If an S3 bucket is specified with the corresponding S3 credentials (See S3 configuration section) then    the output is written to S3.</p> </li> <li> <p><code>runid</code>: The run ID will be part of the output path and allows to distinguish between different pipeline configurations that were used for the same dataset.</p> </li> <li> <p><code>logDir</code>: A path to a directory which is used to store log files.</p> </li> <li> <p><code>scratch</code>: The scratch value can be either <code>false</code> or a path on a worker node. If a path is set, then the nextflow process in <code>slurm</code> mode is executed on the provided path.     If the standard mode is used, then the parameter is ignored.</p> </li> <li> <p><code>steps</code>: Steps allows to specify multiple pipeline modules for running the toolkit. We distinguish between two modes. You can either run one tool of    the pipeline or the whole pipeline with different configurations.</p> </li> <li> <p><code>databases</code>: This parameter specifies a place where files are downloaded to. If the <code>slurm</code> profile is used and databases should be downloaded, the path should point to a folder      which is not shared between the worker nodes (to reduce I/O on the shared folder resulting in a better performance). If this parameter is provided, the toolkit will create the specified     directory. If all your databases have already been extracted beforehand, you can simply omit this parameter.</p> </li> <li> <p><code>publishDirMode</code>: (optional) Per default results are symlinked to the chosen <code>output</code> directory. This default mode can be changed with this parameter.     A useful mode is \"copy\", to copy results instead of just linking them. Other modes to choose from here.  </p> </li> <li> <p><code>skipVersionCheck</code>: The toolkit is regurarly tested against a set of Nextflow versions. Setting the <code>--skipVersionCheck</code> allows you to use the toolkit with Nextflow versions    that were not tested.</p> </li> <li> <p><code>s3SignIn</code>: If your input data (not the databases) is not publicly accessible via S3, then you will have to set the <code>s3SignIn</code> parameter to <code>true</code>.</p> </li> </ul>"},{"location":"pipeline_configuration/#s3-configuration","title":"S3 Configuration","text":"<p>All module inputs and outputs can be used in conjunction with S3. If you want to set a custom S3 configuration setting (i.e. custom S3 endpoint), you will have to modify the aws client parameters  with \" -c \".</p> <p>Example: <pre><code>aws {\n    client {\n      s_3_path_style_access = true\n      maxParallelTransfers = 28 \n      maxErrorRetry = 10\n      protocol = 'HTTPS'\n      endpoint = 'https://openstack.cebitec.uni-bielefeld.de:8080'\n      signerOverride = 'AWSS3V4SignerType'\n    }\n}\n</code></pre></p> <p>In addition you will have to set a Nextflow Secret with the following keys:</p> <pre><code>nextflow secrets set S3_ACCESS xxxxxxxxx\nnextflow secrets set S3_SECRET xxxxxxxxx\n</code></pre> <p><code>S3_ACCESS</code> corresponds to the aws S3 access key id and <code>S3_SECRET</code> is the aws S3 secret key. If your input data (not the databases) is publicly available then you have to set <code>s3SignIn:</code> to <code>false</code> in your config file. Please note that for using databases you have to provide an additional aws credentials file (see database section). </p>"},{"location":"pipeline_configuration/#configuration-of-input-parameters-of-the-full-pipeline-mode","title":"Configuration of input parameters of the full pipeline mode","text":""},{"location":"pipeline_configuration/#paired-end-input","title":"Paired End Input","text":"<p>The input should be a path to a tsv file containing a sample id, as well as a path to the left and right read.</p> <p>Example: <pre><code>input:\n  paired:\n    path: \"test_data/fullPipeline/reads_split.tsv\"\n</code></pre></p>"},{"location":"pipeline_configuration/#nanopore-input","title":"Nanopore Input","text":"<p>For Nanopore data a seperate input file should be specified.</p> <pre><code>input:\n  ont:\n    path: \"test_data/fullPipeline/ont.tsv\"\n</code></pre>"},{"location":"pipeline_configuration/#generic-sra","title":"Generic SRA","text":"<p>The toolkit is able to fetch fastq files based on SRA run accession ids from the NCBI or from a mirror based on S3:</p> <pre><code>input:\n  SRA:\n    pattern:\n      ont: \".+[^(_1|_2)].+$\"\n      illumina: \".+(_1|_2).+$\"\n    S3:\n      path: test_data/SRA/samples.tsv \n      bucket: \"s3://ftp.era.ebi.ac.uk\" \n      prefix: \"/vol1/fastq/\"\n      watch: false\n      patternONT: \".+[^(_1|_2)].+$\"\n      patternIllumina: \".+(_1|_2).+$\"\n</code></pre> <p>where:   * <code>path</code> is the path to a file containing a column with <code>ACCESSION</code> as header. The <code>ACCESSION</code> column contains either SRA run or study accessions.</p> <ul> <li> <p><code>bucket</code> is the S3 Bucket hosting the data.</p> </li> <li> <p><code>prefix</code> is the path to the actual SRA datasets.</p> </li> <li> <p><code>watch</code> if true, the file specified with the <code>path</code> attribute is watched and every time a new SRA run id is      appended, the pipeline is triggered. The pipeline will never finish in this mode. Please note that watch currently only works      if only one input type is specified (e.g \"ont\" or \"paired\" ...)</p> </li> <li> <p><code>patternONT</code> and <code>patternIllumina</code> are patterns that are applied on the specified mirror in order to select the correct input files.</p> </li> </ul>"},{"location":"pipeline_configuration/#ncbi-sra","title":"NCBI SRA","text":"<p>With the following mode SRA datasets can directly be fetched from SRA.</p> <pre><code>input:\n  SRA:\n    pattern:\n      ont: \".+[^(_1|_2)].+$\"\n      illumina: \".+(_1|_2).+$\"\n    NCBI:\n      path: test_data/SRA/samples.tsv\n</code></pre>"},{"location":"pipeline_configuration/#database-input-configuration","title":"Database input configuration","text":"<p>Whenever a database field can be specified as part of the tool configuration (such as in gtdb or checkm), you are able to provide different methods to fetch the database. In all settings, please make sure that the file has the same ending (e.g. .zip, .tar.gz) as specified in the corresponding tool section. In addition, as database names are used to name results with which they were created, said database names should contain the respective database number or date of creation. With this every result can be linked to one exact database version to clarify results.  Except for the <code>extractedDBPath</code> parameter, all other input types (https, s3,...) will download the database to the folder specified in the <code>database</code> parameter.</p>"},{"location":"pipeline_configuration/#extracted-database-path","title":"Extracted Database Path","text":"<p>If you have already downloaded and extracted the database, you can specify the path using the <code>extractedDBPath</code> parameter. This setting is available in standard and slurm mode. In slurm mode the path can point to a db on the worker node.</p> <p>Example: <pre><code>database:\n  extractedDBPath: /vol/spool/gtdb/release202\n</code></pre></p>"},{"location":"pipeline_configuration/#https-download","title":"HTTPS Download","text":"<p>The toolkit is able to download and extract the database, as long as the file ending equals the one specified in the corresponding tool section (.zip, tar.gz, tar.zst) This setting is available in standard and slurm mode. </p> <p>Example: <pre><code>database:\n  download:\n    source: 'https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdb.tar.gz'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n</code></pre></p>"},{"location":"pipeline_configuration/#local-file-path","title":"Local File Path","text":"<p>This setting allows you to reuse an already downloaded database. </p> <p>Example: <pre><code>database:\n  download:\n    source: '/vol/spool/gtdb.tar.gz'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n</code></pre></p>"},{"location":"pipeline_configuration/#s3-download","title":"S3 Download","text":"<p>You can specify an S3 link and configure the S3 call via the <code>s5cmd.params parameter. The</code>s5cmd.params` parameter allows you to set any setting available of the s5cmd commandline tool.  If you need credentials to access your databases, you can set them via the Nextflow secrets mechanism. The correct key name for the access and secret key can be found in the corresponding database section.</p> <p>In the following example the compressed file will be downloaded and extracted.</p> <p>Example for publicly available compressed database: <pre><code>database:\n  download:\n    source: 's3://databases/gtdb.tar.gz'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n    s5cmd:\n      params: '--retry-count 30 --no-sign-request --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080'\n</code></pre></p> <p>If your database is already extracted and available via S3, you can specify the S3 link using a wildcard as in the next example.</p> <pre><code>database:\n  download:\n    source: 's3://databases/gtdb/*'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n    s5cmd:\n      params: '--retry-count 30 --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080'\n</code></pre>"},{"location":"pipeline_configuration/#updating-database-md5sums","title":"Updating Database MD5SUMs","text":"<p>The md5sum is computed over all md5sums of all files of the extracted database. If you need to update the md5sum because you updated your database you have to download the database  and run the following command</p> <pre><code>find /path/to/db -type f -exec md5sum {} \\; | sort | cut -d ' ' -f 1 | md5sum | cut -d ' ' -f 1\n</code></pre>"},{"location":"pipeline_configuration/#database-download-strategy","title":"Database Download strategy","text":"<p>The toolkit allows to download databases on multiple nodes and tries to synchronize the download process between multiple jobs on a node. However not all possible combinations of profiles and download types are reasonable.</p> PROFILE Download to Shared  NFS Download to worker scratch dir Reuse extracted directory STANDARD :material-check: :material-close: :material-check: SLURM :material-check-all: :material-check: :material-check:  On scratch and nfs dir"},{"location":"pipeline_configuration/#optional-configuration-of-computational-resources-used-for-pipeline-runs","title":"Optional configuration of computational resources used for pipeline runs","text":"<p>The toolkit uses the following machine types (flavors) for running tools. All flavors can be optionally adjusted by modifying the cpus and memory (in GB) parameters. If for example the largest flavor is not available in the infrastructure, <code>cpus</code> and <code>memory</code> parameters can be modified to fit the highmemMedium flavor. If larger flavors are available, it makes especially sense to increase the <code>cpus</code> and <code>memory</code> values of the <code>large</code> flavor to speed up for example assembly and read mapping.</p> <p>Example Configuration:</p> <pre><code>resources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Additional flavors can be defined that can be used by methods that dynamically compute resources on tool error (see assembly module section).</p> <p>Example:</p> <pre><code>resources:\n  xlarge:\n    cpus: 56\n    memory: 512\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>The full pipeline mode is able to predict the memory consumption of some assemblers (see assembly module section).</p>"},{"location":"pipeline_configuration/#fragment-recruitment-for-unmapped-reads-configuration","title":"Fragment Recruitment for unmapped reads Configuration","text":"<p>Reads that could not be mapped back to a MAG can be used for fragment recruitment. A list of genomes can be provided in the fragmentRecruitment part.  Matched reference genomes are included in all other parts of the remaining pipeline. Look out for their specific headers to differentiate results based on real assembled genomes and the reference genomes.</p>"},{"location":"pipeline_specification/","title":"Pipeline Specification","text":""},{"location":"pipeline_specification/#output-and-best-practice","title":"Output and best practice","text":""},{"location":"pipeline_specification/#motivation","title":"Motivation","text":"<ul> <li> <p>The output section is a collection of <code>best practices</code> for storing results of the <code>meta-omics-toolkit</code> output. The definitions are motivated by the fact that the pipeline will be continuously updated and results of different pipeline versions and modes must be differentiated.</p> </li> <li> <p>The idea is to run the pipeline on results of previous runs.</p> </li> </ul>"},{"location":"pipeline_specification/#rules-for-dataset-output","title":"Rules for dataset output","text":"<p>Outputs are produced by using the <code>publish dir</code> directive.</p> <p><pre><code>DATASET_ID/RUN_ID/MODULE/VERSION/TOOL/\n</code></pre> where    * <code>DATASET_ID</code> specifies the ID of a dataset such as the SRA run ID.    * <code>RUN_ID</code> specifies one possible run of the full or partial pipeline. The <code>RUN_ID</code> identifier can be any user provided identifier to keep track of multiple pipeline runs.    * <code>MODULE</code> specifies the name of the pipeline module (e.g. binning).    * <code>VERSION</code> specifies the module version number which follows semantic versioning (1.2.0).    * <code>TOOL</code> specifies the name of the tool that is executed as part of the module (e.g <code>megahit</code> of the assembly module).</p> <p>It is suggested that a RUN_ID output should never contain multiple versions of the same module. E.g.:  <code>DATASET_ID/1/Binning/1.2.0/metabat</code> and <code>DATASET_ID/1/Binning/1.3.0/metabat</code>.</p> <p>If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) alreads contains the output of an older version of run (B), then a new RUN_ID folder must be created.</p> <p>If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) does not contain the output of an older version of run (B), then the existing RUN_ID folder must be reused.</p>"},{"location":"pipeline_specification/#run-versioning","title":"Run Versioning","text":"<p>Every dataset must contain a <code>TOOL</code> folder called <code>config</code>. The <code>config</code> folder contains descriptions of the parameters and the version used for the specific pipeline run.</p>"},{"location":"pipeline_specification/#examples","title":"Examples","text":""},{"location":"pipeline_specification/#example-1","title":"Example 1:","text":"<p>We assume that the following folder already exists:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n</code></pre> <p>If the MODULE output does not contain a BINNING output then the existing RUN folder must be reused:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n/SRA1/1/BINNING/0.3/METABAT\n</code></pre>"},{"location":"pipeline_specification/#example-2","title":"Example 2:","text":"<p>We assume that the following folders already exists:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n/SRA1/1/BINNING/0.3/METABAT\n</code></pre> <p>If the MODULE output does contain a BINNING output then a new RUN folder must be created:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n/SRA1/1/BINNING/0.3/METABAT\n/SRA1/2/BINNING/0.4/METABAT\n</code></pre>"},{"location":"pipeline_specification/#rules-for-aggregated-output","title":"Rules for aggregated output","text":"<p>Aggregates outputs in the same way as dataset outputs are produced, by using the publish dir directive with the only difference that no sample identifier are used and the path starts with <code>AGGREGATED</code>.</p> <p>Example:</p> <pre><code>AGGREGATED/RUN_ID/MODULE/VERSION/TOOL/\n</code></pre>"},{"location":"modules/annotation/","title":"Annotation","text":"<p>The annotation module is able to predict genes and annotate those based on Prokka and a set of user provided databases. A user can add additional formatted databases as part of the configuration by adding a key (Example: <code>kegg</code> ) with  a possible download strategy. See database section for possible download strategies. In addition, the resistance gene identifier is executed by default.</p>"},{"location":"modules/annotation/#input","title":"Input","text":"CommandConfiguration FileTSV Table <pre><code>-entry wAnnotateLocal -params-file example_params/annotation.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"modules/annotation/#databases","title":"Databases","text":""},{"location":"modules/annotation/#mmseqs2","title":"MMseqs2","text":"<p>MMseqs2 needs a combination of different data, index and dbtype files as \"one\" database, be it in- or output. See MMseqs2 database for more information. As multiple and in most cases, big files are used, tar and zstd are utilized to compress and transport files. Input databases have to be compressed by these and need to end with <code>.tar.zst</code>. Naming inside an archive is irrelevant, as databases are picked automatically. Multiple databases per one archive are not supported, one archive, one database. If the database also includes a taxonomy  as described here, it can also be used for taxonomic classifications with MMseqs2 - Taxonomy. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_db_ACCESS XXXXXXX\nnextflow secrets set S3_db_SECRET XXXXXXX\n</code></pre> <p>where <code>db</code> is the name of the database that you use in your config file. Example:</p> <pre><code>....\n      vfdb:\n        params: ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3'\n        database:\n          download:\n            source: s3://databases/vfdb_full_2022_07_29.tar.zst\n            md5sum: 7e32aaed112d6e056fb8764b637bf49e\n            s5cmd:\n              params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" \n....\n</code></pre> <p>Based on these settings, you would set the following secret:</p> <pre><code>nextflow secrets set S3_vfdb_ACCESS XXXXXXX\nnextflow secrets set S3_vfdb_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#keggfromblast","title":"KEGGFromBlast","text":"<p>KeGGFromBlast is only executed if genes are searched against a KEGG database. There must be a <code>kegg</code> identifier (see example configuration file) in the annotation section. KeGGFromBlast needs a kegg database as input which must be a tar.gz file. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_kegg_ACCESS XXXXXXX\nnextflow secrets set S3_kegg_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#mmseqs-taxonomy","title":"MMSeqs Taxonomy","text":"<p>If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_TAX_db_ACCESS XXXXXXX\nnextflow secrets set S3_TAX_db_SECRET XXXXXXX\n</code></pre> <p>where <code>db</code> is the name of the database that you use in your config file. Example:</p> <pre><code>....\n    mmseqs2_taxonomy:\n      gtdb:\n        params: ' --orf-filter-s 1 -e 1e-15'\n        ramMode: false\n        database:\n          download:\n            source: s3://databases/gtdb_r214_1_mmseqs.tar.gz\n            md5sum: 3c8f12c5c2dc55841a14dd30a0a4c718\n            s5cmd:\n              params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" \n....\n</code></pre> <p>Based on these settings, you would set the following secrets:</p> <pre><code>nextflow secrets set S3_TAX_gtdb_ACCESS XXXXXXX\nnextflow secrets set S3_TAX_gtdb_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#rgi","title":"RGI","text":"<p>RGI needs a CARD database which can be fetched via this link:  https://card.mcmaster.ca/latest/data. The compressed database must be a tar.bz2 file.  See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_rgi_ACCESS XXXXXXX\nnextflow secrets set S3_rgi_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#output","title":"Output","text":""},{"location":"modules/annotation/#mmseqs2_1","title":"MMseqs2","text":"<p>Calculated significant matches of a nucleotide/protein query which was compared against a user provided set of databases.</p>"},{"location":"modules/annotation/#mmseqs2-taxonomy","title":"MMseqs2 - Taxonomy","text":"<p>By identifying homologous through searches against a provided MMseqs2 taxonomy-database, MMseqs2 can compute the lowest common ancestor.  This lowest common ancestor is a robust taxonomic label for unknown sequences. These labels are presented in form of an <code>*.taxonomy.tsv</code> file, a <code>*.krakenStyleTaxonomy.out</code> formatted in accordance to the KRAKEN tool outputs and an interactive KRONA plot in form of a html website <code>*.krona.html</code>.</p>"},{"location":"modules/annotation/#prokka","title":"Prokka","text":"<p>Prokka computes <code>*.err</code>, <code>*.faa</code>, <code>*.ffn</code>, <code>*.fna</code>, <code>*.fsa</code>, <code>*.gbk</code>, <code>*.gff</code>, <code>*.sqn</code>, <code>*.tbl</code>, <code>*.tbl</code> for every bin. <code>*.gbk</code> and <code>*.sqn</code> are skipped per default, since tbl2asn runs for quite a while! If you need those files generated by prokka, include: <code>--tbl2asn</code> in the prokka parameters to enable it. Details of all files can be read on the Prokka page. In addition, it also computes a summary tsv file which adheres to the magAttributes specification.</p>"},{"location":"modules/annotation/#keggfromblast_1","title":"KEGGFromBlast","text":"<p>Result <code>*.tsv</code> file filled with KEGG information (like modules, KO's, ...) which could be linked to the input hits.</p>"},{"location":"modules/annotation/#resistance-gene-identifier-rgi","title":"Resistance Gene Identifier (rgi)","text":"<p>The <code>*rgi.tsv</code> files contain the found CARD genes.</p>"},{"location":"modules/assembly/","title":"Assembly","text":""},{"location":"modules/assembly/#input","title":"Input","text":"Command for short read data with optional single end readsCommand for long read dataMegahit Configuration FileMetaspades Configuration FileMetaFlye Configuration FileTSV Table Short ReadTSV Table Nanopore <pre><code>-entry wShortReadAssembly -params-file example_params/assembly.yml\n</code></pre> <pre><code>-entry wOntAssembly -params-file example_params/assemblyONT.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"modules/assembly/#output","title":"Output","text":"<p>The output is a gzipped fasta file containing contigs.</p>"},{"location":"modules/assembly/#megahit","title":"Megahit","text":""},{"location":"modules/assembly/#error-handling","title":"Error Handling","text":"<p>On error with exit codes ([-9, 137, 247]) (e.g. due to memory restrictions), the tool is executed again with higher cpu and memory values. The memory and cpu values are in case of a retry selected based on the flavor with the next higher memory value. The highest possible cpu/memory value is restricted by the highest cpu/memory value of all flavors defined in the resource section  (see global configuration section). </p>"},{"location":"modules/assembly/#peak-memory-usage-prediction","title":"Peak memory usage prediction","text":"<p>Memory consumption of an assembler varies based on diversity. We trained a machine learning model on kmer frequencies and the nonpareil diversity index in order to be able to predict the memory peak consumption of megahit in our full pipeline mode. The required resources in order to run the assembler are thereby fitted to the resources that are actually needed for a specific dataset. If this mode is enabled then Nonpareil and kmc that are part of the quality control module are automatically executed before the assembler run.  </p> <p>Please note that this mode is only tested for Megahit with default parameters and the meta-sensitive mode (<code>--presets meta-sensitive</code>).</p> <pre><code>  resources:\n    RAM: \n      mode: MODE\n      predictMinLabel: LABEL\n</code></pre> <p>where      * MODE can be either 'PREDICT' for predicting memory usage or 'DEFAULT' for using a default flavor defined in the resources section.</p> <pre><code>* LABEL is the flavor that will be used if the predicted RAM is below the memory value defined as part of the LABEL flavor. It can also be set to AUTO to always use the predicted flavor.\n</code></pre>"},{"location":"modules/cooccurrence/","title":"Cooccurrence","text":"<p>The Cooccurrence module builds a cooccurrence network where each node is a MAG and every edge represents an association between them. The network can be inferred based on correlation or inverse covariance estimation by SPIEC-EASI. SPIEC-EASI is executed multiple times based on different parameter settings in order to find the most stable network. In addition, it is possible to compute multiple metrics for every edge based on genome-scale metabolic models and the SMETANA metrics. </p> <pre><code>-entry wCooccurrence -params-file example_params/cooccurrence.yml\n</code></pre>"},{"location":"modules/cooccurrence/#input","title":"Input","text":"CommandConfiguration File for CooccurrenceTSV TableGTDB TSV TableConfiguration File for analyzing edges in Cooccurrence GraphGTDB TSV for analyzing EdgesModel TSV for computing Metabolomics Metrics on Edges <pre><code>-entry wCooccurrence -params-file example_params/cooccurrence.yml\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> Contains abundance values of mags per sample.</p> <p><pre><code>\n</code></pre> GTDB assignment of all samples that were produced by magAttributes module.</p> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> GTDB assignment of all samples that were produced by the magAttributes module.</p> <pre><code>\n</code></pre> <p>The following parameters can be configured:</p> <ul> <li> <p>metabolicEdgeBatches: Batches of edges that are provided as input to SMETANA.</p> </li> <li> <p>metabolicEdgeReplicates: Number of replicates per edge that should be computed.</p> </li> </ul>"},{"location":"modules/cooccurrence/#output","title":"Output","text":"<ul> <li> <p>output_raw.graphml: Cooccurrence network unfiltered in graphml format</p> </li> <li> <p>output.graphml: Filtered cooccurrence network in graphml format</p> </li> <li> <p>edges_index.tsv: Edges of the graph</p> </li> <li> <p>edgeAttributes.tsv: Edge attributes of the graph containing metrics computed by SMETANA.</p> </li> </ul>"},{"location":"modules/cooccurrence/#spiec-easi","title":"SPIEC-EASI","text":"<ul> <li>stability.txt: Network stability estimation</li> </ul>"},{"location":"modules/dereplication/","title":"Dereplication","text":""},{"location":"modules/dereplication/#input","title":"Input","text":"CommandConfiguration FileTSV Table <pre><code>-entry wDereplication -params-file example_params/dereplication.yml\n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> Must include the columns <code>DATASET</code>, <code>BIN_ID</code>, <code>PATH</code>, <code>COMPLETENESS</code>, <code>CONTAMINATION</code>, <code>COVERAGE</code>, <code>N50</code> and <code>HETEROGENEITY</code>.  Completeness and contamination can be used for filtering (see <code>params-file</code>). <code>N50</code>, <code>COVERAGE</code> and <code>HETEROGENEITY</code> are used for selecting the representative of every cluster. You can set values of these columns to zero if data is not available or if you don't want the representative selection to be influenced by theses columns. Make sure that <code>BIN_ID</code> is a unique identifier.</p>"},{"location":"modules/dereplication/#output","title":"Output","text":"<p>The output tsv file (<code>clusters.tsv</code>in the cluster<code>s folder) contains the columns</code>CLUSTER<code>,</code>GENOME<code>and</code>REPRESENTATIVE<code>where</code>CLUSTER<code>identifies a group of genomes,</code>GENOME<code>represents the path or link of a genome and</code>REPRESENTATIVE<code>is either 0 or 1 (selected as representative). If</code>sans<code>is specified in the configuration file (see examples folder), then [SANS](https://gitlab.ub.uni-bielefeld.de/gi/sans) is used to dereplicate the genomes of every cluster that was reported by the previous step.  The SANS output can be found in the</code>sans` folder.</p>"},{"location":"modules/fragment_recruitment/","title":"Run Fragment Recruitment","text":"<p>The fragment recruitment module can be used to find genomes in a set of read datasets.</p> <p>Note: This module only supports illumina data. </p>"},{"location":"modules/fragment_recruitment/#input","title":"Input","text":"CommandConfiguration file for fragment recruitment via mash screen and BWAConfiguration file for fragment recruitment via mash screen and BowtieInput TSV file for genomesInput TSV file for paired end readsInput TSV file for single end reads <pre><code>-entry wFragmentRecruitment -params-file example_params/fragmentRecruitment.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>NOTE! The file names of all provided genomes must be unique.</p> <p>The following parameters can be configured:</p> <ul> <li> <p>mashDistCutoff: All hits below this threshold are  discarded.</p> </li> <li> <p>mashHashCutoff: All hits that have a lower count of matched minimum hashes are discarded.</p> </li> <li> <p>coveredBasesCutoff: Number of bases that must be covered by at least one read. By how many reads     the bases must be covered can be configured via the coverm setting (coverm: \"  --min-covered-fraction 0  \").</p> </li> </ul>"},{"location":"modules/fragment_recruitment/#output","title":"Output","text":"<p>The module outputs mash screen and bowtie alignment statistics.  Furthermore, the module provides a coverm output which basically reports all metrics about the found genomes (e.g covered bases,length, tpm, ...).</p>"},{"location":"modules/magAttributes/","title":"MagAttributes","text":""},{"location":"modules/magAttributes/#input","title":"Input","text":"CommandConfiguration FileMAGs TSV Table <pre><code>-entry wMagAttributes -params-file example_params/magAttributes.yml \n</code></pre> <pre><code>\n</code></pre> <p><pre><code>\n</code></pre> Must include at least <code>DATASET</code> identifier and mag specific <code>PATH</code> and <code>BIN_ID</code> column.</p>"},{"location":"modules/magAttributes/#databases","title":"Databases","text":"<p>Checkm and GTDB need their databases as input. See database section for possibly download strategies. The GTDB and Checkm compressed databases must be tar.gz files. If you provide the extracted version of GTDB using the <code>extractedDBPath</code> parameter, please specify the path to the <code>releasesXXX</code> directory (e.g. \"/vol/spool/gtdb/release202\").</p> <p>If you need credentials to access your files via S3 then please use the following command:</p> <p>For GTDB: <pre><code>nextflow secrets set S3_gtdb_ACCESS XXXXXXX\nnextflow secrets set S3_gtdb_SECRET XXXXXXX\n</code></pre></p> <p>For Checkm: <pre><code>nextflow secrets set S3_checkm_ACCESS XXXXXXX\nnextflow secrets set S3_checkm_SECRET XXXXXXX\n</code></pre></p>"},{"location":"modules/magAttributes/#output","title":"Output","text":""},{"location":"modules/magAttributes/#gtdbtk","title":"GTDBTk","text":"<p>All GTDB files include the GTDB specific columns in addition to a <code>SAMPLE</code> column (<code>SAMPLE_gtdbtk.bac120.summary.tsv</code>, <code>SAMPLE_gtdbtk.ar122.summary.tsv</code>). In addition, this module produces a file <code>SAMPLE_gtdbtk_CHUNK.tsv</code> that combines both files and adds a <code>BIN_ID</code> column that adheres to the magAttributes specification</p>"},{"location":"modules/magAttributes/#checkm-and-checkm2","title":"Checkm and Checkm2","text":"<p>The Checkm and Checkm2 output adheres to the magAttributes specification and adds a <code>BIN_ID</code> and <code>SAMPLE</code> column to the output file. If Checkm2 and Checkm are both specified in the config file then only the Checkm2 results are used for downstream pipeline steps.</p>"},{"location":"modules/metabolomics/","title":"Metabolomics","text":"<p>The metabolomics module runs genome scale metabolic modeling analysis based on a supplied genome or directly on its proteins. The module is able to use gapseq and carveme for analysing genomes and carveme for analysing predicted proteins which depends on the configuration you provide as input.</p> <p>Note: If carvem is specificed in fullPipeline mode then carveme is executed with proteins as input.</p> <p>All generated models are used for further downstream analysis such as the \"Minimum Resource Overlap\" computation by smetana.</p>"},{"location":"modules/metabolomics/#input","title":"Input","text":"CommandConfiguration file for providing genomes <pre><code>-entry wMetabolomics -params-file example_params/metabolomics\n</code></pre> <pre><code>\n</code></pre> <p>Almost all tools of this module are using linear programming solvers. The tool developers are recommending the use of the cplex solver that is included in the IBM ILOG CPLEX Optimization Studio which is free for students and academics through the IBM Academic Initiative programm.  Since the toolkit uses docker images that are downloaded from public Docker Hub repositories and the cplex license is not allowed to be distributed, we prepared a Dockerfile that allows you to build your own local docker image with all metabolomics specific tools installed. Just copy your cplex binary to the cplex docker folder and build your own docker image. You can override all existing images via the command line.</p> <p>In the following example your the image name is metabolomics:0.1.0:</p> <ul> <li><code>--gapseq_image=metabolomics:0.1.0</code> (Optional)</li> <li><code>--smetana_image=metabolomics:0.1.0</code> (Required)</li> <li><code>--carveme_image=metabolomics:0.1.0</code> (Optional. Carveme is not able to detect the solver automatically. Please specify <code>--solver</code> in the configuration file if you want to use the scip solver.)</li> <li><code>--memote_image=metabolomics:0.1.0</code> (Optional. Memote is not able to detect the solver automatically. Please specify <code>--solver</code> in the configuration file if you are not using the glpk solver.)</li> </ul> <p>For gapseq and memote we are using a publicly available docker image that uses the freely available glkp solver which means that you don't have to provide this parameter. If you want to build your own image, please use the <code>beforeProcessScript</code> parameter. This parameter expects a bash script that accepts the docker image name as a parameter. The script is executed right before the actual docker image is started.  You could for example provide a script that builds the actual image right before running the tool on the VM.  It would be also possible to push the docker image to a private dockerhub repository and login to your docker account via this script. We provide two example template scripts in the cplex folder. Please note that in both cases you distribute the docker image with your cplex binary on all machines where you run the toolkit. If you login to dockerhub then your credentials will saved on the VM. If you are not the only docker user on the machine we do not recommend this approach! </p>"},{"location":"modules/metabolomics/#output","title":"Output","text":""},{"location":"modules/metabolomics/#gapseq-carveme","title":"GapSeq / CarveMe","text":"<p>Both tools are generating genome scale metabolic reconstruction models (<code>*.xml</code>).  All models are translated to json format and substrats, products and reactions are saved in distinct files.</p>"},{"location":"modules/metabolomics/#memote","title":"Memote","text":"<p>Memote tests metabolic reconstruction models and therefore produces a machine readable json file  (<code>*_report.json.gz</code>) and a human readable tsv (<code>*_metrics.tsv</code>) and html (<code>*_report.html</code>) file.</p>"},{"location":"modules/metabolomics/#smetana","title":"Smetana","text":"<p>Smetana is used for analysing possible interactions in microbial communities. Smetana<code>s global and detailed modes  are executed per sample. The Smetana output is saved in</code>_detailed.tsv<code>and</code>_global.tsv`.</p>"},{"location":"modules/plasmids/","title":"Plasmids","text":"<p>The plasmid module is able to identify contigs as plasmids and also to assemble plasmids from the samples fastq data. The module is executed in two parts. In the first part contigs of a metagenome assembler are scanned for plasmids. In the second part a plasmid assembler is used to assemble circular plasmids out of raw reads. All plasmid detection tools are executed on the circular assembly result and on the contigs of the metagenome assembler. Just the filtered sequences are used for downstream analysis. </p> <p>The identification of plasmids is based on the combined result of tools which have a <code>filter</code> property assigned. Results of all tools that have the <code>filter</code> property set to true are combined either by a logical <code>OR</code> or by a logical <code>AND</code>. </p> <p>Example for the <code>OR</code> and <code>AND</code> operations:  Let's assume that we have three plasmid detection tools (t1, t2, t3) that have four contigs (c1, c2, c3, c4) as input. Let's further assume that c1 and c2 are detected by all tools as contigs and c3 and c4 are only detected by t1 and t2. By using an <code>AND</code> only c1 and c2 are finally reported by the module as plasmids. By using an <code>OR</code> all contigs would be annotated as plasmids. </p> <p>It is also possible to simply run a tool without using its result as filter by setting <code>filter</code> to <code>false</code>. If a tool should not be executed then the tool section should be removed. Only the detected plasmids will be used for downstream analysis.</p> <p>For running a plasmid assembly we suggest running the full pipeline mode with the enabled plasmids module. See input example configuration files. The read mapper can either be Bowtie or Bwa for Illumina and minimap for long reads.  </p>"},{"location":"modules/plasmids/#input","title":"Input","text":"CommandConfiguration file for full pipeline mode with plasmids detectionsConfiguration file for plasmids module onlyTSV Table <pre><code>-entry wPlasmidsPath -params-file example_params/plasmids.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"modules/plasmids/#databases","title":"Databases","text":"<p>The plasmid module needs the following compressed database file formats: </p>"},{"location":"modules/plasmids/#viralverifyplasmid","title":"ViralVerifyPlasmid","text":"<p>ViralVerifyPlasmid needs a recent pfam-A database in .gz format. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_ViralVerifyPlasmid_ACCESS XXXXXXX\nnextflow secrets set S3_ViralVerifyPlasmid_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#mobtyper","title":"MobTyper","text":"<p>Database was generated by gzipping the output of mob_init. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_MobTyper_ACCESS XXXXXXX\nnextflow secrets set S3_MobTyper_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#platon","title":"Platon","text":"<p>The tar gzipped database for running platon can be fetched from the Platon github page. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_Platon_ACCESS XXXXXXX\nnextflow secrets set S3_Platon_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#plsdb","title":"PLSDB","text":"<p>PLSDB Database is available via this link: https://ccb-microbe.cs.uni-saarland.de/plsdb/plasmids/download/plasmids_meta.tar.bz2. All files except .tsv and .msh were deleted from the compressed package. See database section for possible download strategies. The compressed database must be a tar.bz2 file.  If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_PLSDB_ACCESS XXXXXXX\nnextflow secrets set S3_PLSDB_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#output","title":"Output","text":""},{"location":"modules/plasmids/#scapp","title":"SCAPP","text":"<p>SCAPP detects plasmid sequences out of the samples assembly graph. It reports sequences as gzipped fasta files (<code>*_plasmids.fasta.gz</code>). A basic statistic (<code>*_plasmids_stats.tsv</code>) per plasmid and a summary statistic (<code>*_plasmids_summary_stats.tsv</code>) over all plasmids is also generated. Coverm coverage metrics are generated for all plasmids. Gene coverage values are generated as part of the annotation module output.</p>"},{"location":"modules/plasmids/#plasclass","title":"PlasClass","text":"<p>PlasClass is able to identify plasmids by using a statistical model that was build using kmer frequencies. It reports gzipped fata files and their probabilities (<code>*_plasclass.tsv</code>).</p>"},{"location":"modules/plasmids/#mobtyper-and-platon","title":"MobTyper and Platon","text":"<p>MobTyper and Platon are using both replicon typing for plasmid detection. (<code>*_mobtyper_results.tsv</code>, <code>*_platon.tsv</code>)</p>"},{"location":"modules/plasmids/#viralverifyplasmid_1","title":"ViralVerifyPlasmid","text":"<p>ViralVerfiy is applying a Naive Bayes classifier (<code>*_viralverifyplasmid.tsv</code>).</p>"},{"location":"modules/plasmids/#plsdb_1","title":"PLSDB","text":"<p>PLSDB includes a curated set of plasmid sequences that were extracted from databases like refseq. The metadata of found sequences are reported in <code>*.tsv</code> and the metadata of the filtered sequences in <code>*_kmerThreshold_X.tsv</code>.</p>"},{"location":"modules/qualityControl/","title":"Quality Control","text":"<p>The quality control module removes adapters, trims and filters short read and long read data.</p>"},{"location":"modules/qualityControl/#input","title":"Input","text":"Command for short read dataCommand for nanopore dataTSV Table short readTSV Table nanopore <pre><code>-entry wShortReadQualityControl -params-file example_params/qc.yml\n</code></pre> <pre><code>-entry wOntQualityControl -params-file example_params/qcONT.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"modules/qualityControl/#output","title":"Output","text":"<p>The output is a gzipped fastq file (short read: <code>SAMPLE_interleaved.qc.fq.gz</code>, long read: <code>SAMPLE_qc.fq.gz</code>) containing trimmed and quality filtered reads.</p>"},{"location":"modules/readMapping/","title":"Read Mapping","text":"<p>Note: This module only supports illumina data. </p>"},{"location":"modules/readMapping/#input","title":"Input","text":"CommandConfiguration FileMAGs TSV TableSamples TSV Table <pre><code>-entry wReadMapping -params-file example_params/readMapping.yml\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"modules/readMapping/#output","title":"Output","text":"<p>The produced output files are the following: count.tsv, mean.tsv, mean_mincov10.tsv, rpkm.tsv, tpm.tsv, trimmed_mean.tsv. The content of the files are produced by coverm. All metrics are explained on the coverm GitHub page: https://github.com/wwood/CoverM .</p>"}]}