{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Metagenomics-Toolkit Introduction The Metagenomics-Toolkit allows you to run either the full pipeline of assembly, binning and many other downstream analysis tasks or individual modules. The toolkit can be configured by providing the module configuration via a yml file and a flag for the corresponding module or full pipeline mode. Options for global pipeline configuration can be viewed here . All tools follow the same error strategy. The execution of a tool is retried three times. If the run fails the fourth time, it will be ignored. If the execution is ignored, the toolkit will continue to run all tools that do not depend on the output of the failed tool run. Exceptions of this handling are specified in the corresponding module section. Note! Please do never place sensitive information in any of the yml configuration files since the configuration is part of the pipeline output. Run Full Pipeline nextflow run main.nf -work-dir /shared/directory/test \\ -profile PROFILE -resume \\ -entry wFullPipeline -params-file example_params/fullPipeline.yml where * /shared/directory/test is a directory that is shared between multiple machines. * PROFILE can be either standard (local use) or slurm depending on which environment the pipeline should be executed. Input Command Configuration File TSV Table Additional S3 Configuration -entry wFullPipeline -params-file example_params/fullPipeline.yml Must include the columns SAMPLE , READS1 and READS2 . SAMPLE must contain unique dataset identifiers without whitespaces or special characters. READS1 and READS2 are paired reads and can be HTTPS URLs, S3 links or files. Nextflow usually stores downloaded files in the work directory. If enough scratch space is available on the worker nodes then this can be prevented by specifying s3 links in the input tsv file and download parameter in the input yaml. S3 TSV Links: Input YAML Output (Overview) In addition to the pipeline module outputs defined in the module section (Dereplication, MagAttributes, etc), the following outputs are produced. quality control (fastq) assembly (contigs) binning (genomes) read mapping (bam files) Optional: Run per sample analysis and the aggregation of per sample seperately There are two ways to execute the toolkit. You can either run all steps in one execution or you run first the per sample analysis (e.g. assembly, binning, annotation, etc.) and afterwards you combine the results (e.g. dereplication, co-occurrence) in a second run. The second option allows you to process multiple samples via independent toolkit executions on different infrastructures and combine all results afterwards. You would first have to run the wFullPipeline mode without dereplication, read mapping and co-occurrence modules and afterwards run the the aggregation as described below: Input Command Configuration File -entry wAggregatePipeline -params-file example_params/fullPipelineAggregate.yml Elastic Metagenomic Browser (EMGB) The output generated by the Metagenomics-Toolkit can be imported into (EMGB)[https://gitlab.ub.uni-bielefeld.de/cmg/emgb/emgb-server]. EMGB allows you to easily explore your metagenomic samples in terms of MAGs, genes and their function. Your configuration must include at least the following analysis steps: * Assembly * Binning * Gene prediction and annotation with Prokka * Gene annotation with MMseqs * Gene taxonomy prediction with MMseqs We offer a script bin/emgb.sh that allows you to export a metagenomics toolkit output folder to emgb compatible json files. Example Call: bash emgb.sh --output=output/test1 --runid=1 --binsdir=output/test1/1/binning/0.5.0/metabat --blastdb=bacmet20_predicted --name=test1 You can get a help page for the necessary arguments by running emgb.sh --help . Caveats The pipeline breaks if --stageInMode is specified with copy .","title":"Home"},{"location":"#metagenomics-toolkit","text":"","title":"Metagenomics-Toolkit"},{"location":"#introduction","text":"The Metagenomics-Toolkit allows you to run either the full pipeline of assembly, binning and many other downstream analysis tasks or individual modules. The toolkit can be configured by providing the module configuration via a yml file and a flag for the corresponding module or full pipeline mode. Options for global pipeline configuration can be viewed here . All tools follow the same error strategy. The execution of a tool is retried three times. If the run fails the fourth time, it will be ignored. If the execution is ignored, the toolkit will continue to run all tools that do not depend on the output of the failed tool run. Exceptions of this handling are specified in the corresponding module section. Note! Please do never place sensitive information in any of the yml configuration files since the configuration is part of the pipeline output.","title":"Introduction"},{"location":"#run-full-pipeline","text":"nextflow run main.nf -work-dir /shared/directory/test \\ -profile PROFILE -resume \\ -entry wFullPipeline -params-file example_params/fullPipeline.yml where * /shared/directory/test is a directory that is shared between multiple machines. * PROFILE can be either standard (local use) or slurm depending on which environment the pipeline should be executed.","title":"Run Full Pipeline"},{"location":"#input","text":"Command Configuration File TSV Table Additional S3 Configuration -entry wFullPipeline -params-file example_params/fullPipeline.yml Must include the columns SAMPLE , READS1 and READS2 . SAMPLE must contain unique dataset identifiers without whitespaces or special characters. READS1 and READS2 are paired reads and can be HTTPS URLs, S3 links or files. Nextflow usually stores downloaded files in the work directory. If enough scratch space is available on the worker nodes then this can be prevented by specifying s3 links in the input tsv file and download parameter in the input yaml. S3 TSV Links: Input YAML","title":"Input"},{"location":"#output-overview","text":"In addition to the pipeline module outputs defined in the module section (Dereplication, MagAttributes, etc), the following outputs are produced. quality control (fastq) assembly (contigs) binning (genomes) read mapping (bam files)","title":"Output (Overview)"},{"location":"#optional-run-per-sample-analysis-and-the-aggregation-of-per-sample-seperately","text":"There are two ways to execute the toolkit. You can either run all steps in one execution or you run first the per sample analysis (e.g. assembly, binning, annotation, etc.) and afterwards you combine the results (e.g. dereplication, co-occurrence) in a second run. The second option allows you to process multiple samples via independent toolkit executions on different infrastructures and combine all results afterwards. You would first have to run the wFullPipeline mode without dereplication, read mapping and co-occurrence modules and afterwards run the the aggregation as described below:","title":"Optional: Run per sample analysis and the aggregation of per sample seperately"},{"location":"#input_1","text":"Command Configuration File -entry wAggregatePipeline -params-file example_params/fullPipelineAggregate.yml","title":"Input"},{"location":"#elastic-metagenomic-browser-emgb","text":"The output generated by the Metagenomics-Toolkit can be imported into (EMGB)[https://gitlab.ub.uni-bielefeld.de/cmg/emgb/emgb-server]. EMGB allows you to easily explore your metagenomic samples in terms of MAGs, genes and their function. Your configuration must include at least the following analysis steps: * Assembly * Binning * Gene prediction and annotation with Prokka * Gene annotation with MMseqs * Gene taxonomy prediction with MMseqs We offer a script bin/emgb.sh that allows you to export a metagenomics toolkit output folder to emgb compatible json files. Example Call: bash emgb.sh --output=output/test1 --runid=1 --binsdir=output/test1/1/binning/0.5.0/metabat --blastdb=bacmet20_predicted --name=test1 You can get a help page for the necessary arguments by running emgb.sh --help .","title":"Elastic Metagenomic Browser (EMGB)"},{"location":"#caveats","text":"The pipeline breaks if --stageInMode is specified with copy .","title":"Caveats"},{"location":"developer_guidelines/","text":"Guidelines Commit and Release Guidelines We are using git-chglog to automatically generate a changelog for the latest released based on our commit messages. Commit messages should follow the following format: feat(scope): feature added in the scope Example: feat(assembly): megahit added feat can be replaced by one of the formats specified in the options sections of the config file (see example below). Scope can for example represent a module, a configuration or a specific document. A new release should be made the following way: Update pipeline version in the nextflow manifest nextflow.config . Create a release on Github. Run git fetch on the master branch to get the latest tag. Run make changelog and paste the output on the Github release section. Versioning Following semantic versioning , we define the configuration input file and the output folder structure as our public API . Changes to the version numbers reflect updates to the config or the output folders and files. The toolkit consists of many modules that can be used in different combinations and because of this flexibility, we had to come up with a detailed versioning system. We version each module separately, as well as the pipeline itself. Module MAJOR version numbers are updated when a module-specific input parameter is updated or the output folder or file structure is changed. All module version numbers can be retrieved by running the toolkit with the wGetModuleVersion entry point and should be reported on the release page. The module version number is incorporated in the output directory (see output specification ) for easier parsing of the output directory. In the following we give examples when to increment which part of the version identifier: Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible changes, as for example modifying the output structure. A script that was build to parse the output structure must be adapted then. MINOR version when you add functionality in a backward compatible manner. One example is adding an additional tool to the module. PATCH version when you make backwards compatible bug fixes. This is necessary when you for example increment the docker container version number that fixes a bug or increases the speed of the tool. The pipeline specific version number defined in the manifest part of the nextflow.config should be changed if either any module specific version number is incremented or any module-independent parameter (e.g. tempdir ) or output structure is changed. Testing Tests for local use are specified in the scripts folder. These scripts are also used as part of the continuous integration tests. If you want to run these scripts locally, you will have to override the paths to the databases you have downloaded: Examples: bash scripts/test_fullPipeline.sh \" --steps.magAttributes.checkm.database=/vol/spool/checkm --steps.magAttributes.gtdb.database=/vol/spool/gtdb/release202 \" bash scripts/test_fragmentRecruitment.sh \" --steps.fragmentRecruitment.frhit.genomes=test/bins/small/bin.*.fa --steps.fragmentRecruitment.frhit.samples=test/reads/small/reads.tsv \" bash scripts/test_dereplication.sh \" --steps.dereplication.pasolli.input=test/bins/small/attributes.tsv \" bash scripts/test_magAttributes.sh \" --steps.magAttributes.input=test/bins/small/attributes.tsv \" Nextflow Versions The toolkit is tested against the lowest and highest Nextflow version number specified in VERSIONS.txt. Modules Functionality is structured in modules (assembly, binning, dereplication, .etc). Each module can have multiple workflows. Every module follows the output definition specified in the output specification document. The name and the version of the module is specified in the modules section of the nextflow.config file. Workflows Worfklow names that can not be used directly and are just meant for internal use should start with an underscore. At least every workflow that can be used by other external workflows should contain a short description of the functionality. Workflow names must start with w . Process Process names should start p . The in- and output of processes should contain a sample and/or a bin and contig id. Custom error strategies that do not follow the strategy defined in nextflow.config, should be documented (see Megahit example). Processes should publish process specific files Processes should publish .command.sh , .command.out , .command.log and .command.err files but never .command.run . In cases where processes process different data but publish it to the same folder these files would be overwritten on every run. For example when Prokka publishes log files of every genome to the same sample directory. For that reason these files need to be renamed, so that their names include a unique id (e.g. bin id). Please output those files to channel with the following entries and connect this channel to the pDumpLogs process that you can import from the utils module: include { pDumpLogs } from ' .. / utils / processes ' ... tuple env ( FILE_ID ), val ( \"${output}\" ), val ( params . LOG_LEVELS . INFO ), file ( \".command.sh\" ), \\ file ( \".command.out\" ), file ( \".command.err\" ), file ( \".command.log\" ), emit : logs Examples can be viewed in the Checkm and Prokka process. Logs Log files should be stored in the user provided logDir directory. Log Level Every configuration file must have a logLevel attribute that can have the following values: ALL = 0 All logs are published INFO = 1 Just necessary logs are published These values can be used in the publish dir directive to enable or disable the output of logs. publishDir params . output , mode : \"${params.publishDirMode}\" , saveAs : { filename -> getOutput ( params . runid , \"pasolli/mash/sketch\" , filename ) }, \\ pattern : \"{**.out,**.err, **.sh, **.log}\" , enabled : params . logLevel <= params . LOG_LEVELS . ALL Furthermore the params.LOG_LEVELS.* parameters can be used inside of a process to enable or disable intermediate results for debugging purposes. In cases where the log is send to the pDumpLogs process (see Process section), you can specify the log level as part of the tuple: tuple env(FILE_ID), val(\"${output}\"), val(params.LOG_LEVELS.INFO), file(\".command.sh\"), \\ file(\".command.out\"), file(\".command.err\"), file(\".command.log\"), emit: logs Time Limit Every process must define a time limit which will never be reached on \"normal\" execution. This limit is only useful for errors in the execution environment which could lead to an endless execution of the process. You can use the setTimeLimit helper method to add a user configurable time limit. Example: time Utils.setTimeLimit(params.steps.qc.fastp, params.modules.qc.process.fastp.defaults, params.resources.highmemMedium) Databases If the same database is downloaded during runtime by multiple processes, it takes up an unnecessary ammount of disc space. One idea is too always use the same place to store these databases. This place should be described in params.databases . If other processes try to use this databases they can look at params.databases on the current machine. If it is present it can be used, if not it should be downloaded. Through this procedure only one copy of each databases is used, which is space-saving. Links to the actual database should contain the database version number or the date of download. Configuration Every process should be configurable by providing a parameters string to the tool in the process. Every module should use the following specification in the configuration file: steps : moduleName : parameter : 42 processName : additionalParams : \" --super-flag \" timeLimit : \"AUTO\" Please check the process chapter regarding possible values for the time limit attribute. Additional params can have a string value (like the example above) that is provided to the tool: pProcess { ... shell : \"\"\" supertool !{params.steps.moduleName.processName.parameter} !{params.steps.moduleName.processName.additionalParams} \"\"\" } The value of the additionalParams key can also be a map if multiple tools are used in the same process: steps : moduleName : parameter : 42 processName : additionalParams : toolNameA : \" -c 84 \" toolNameB : \" --super-flag \" parameter fields can hold hardcoded parameters that hold a defined value like a number that should not be a string. One use case of those parameters is that they can be reused for multiple tools. Example: pProcess { ... shell: \"\"\" toolNameA --super-specific-number-flag !{params.steps.moduleName.parameter} toolNameB --similar-flag-to-toolA !{params.steps.moduleName.parameter} \"\"\" } Internal Configuration The _wConfigurePipeline workflow in the main.nf file should be used for setting pipeline parameters that are need for fullfilling the user provided configuration. Example: Lets assume the user enables the plasmid module. In that case it is mandatory that the assembler produces a fastg file independend of the user provided settings of the assembler. In that case the fastg parameter of any assembler will be set to true by the _wConfigurePipeline method. Toolkit Docker Images Dockerfiles of Docker images that are build by toolkit developers can be found in the docker directory. The name of the directory (i.e.: toolkit-python-env in docker/toolkit-python-env ) is used for the docker image name. All images belong to the metagenomics quay.io organisation which is owned by the Computational Metagenomics group in Bielefeld. A docker repository in the metagenomics orginsation must be created by the organisation owner, before the actual image can be build. The version of the image specified in the VERSION file (i.e. docker/toolkit-python-env/VERSION ) is used for the image tag ( metagenomics/toolkit-python-env:VERSION ). An image build is only triggered if the version in the VERSION file is updated on the dev or master branch. Wiki For building the documentation we are using mkdocs in combination with mkdocs-material and a plugin for building static single page html files. The wiki HTML files are uploaded to S3 storage on pull request merge events in the master and dev branch (see Makefile commands using make help ). You can work on these html files locally by running make dev_wiki . But please note that by build the static html file for upload, the navigation might change. You can view the final html file by building the html file (see Makefile make help ). Utils We do not want to duplicate code and thats why we should store methods in the lib/Utils.groovy file. The Utils class can be used in any module. Database Download This section explains how a developer is able to implement the database download strategy as explained in the user documentation . Example implementations can be found in the gtdb, checkm or rgi scripts. The first step is to check if the user provides an already extracted database: DB_PATH = \"\" if [ -z \"!{EXTRACTED_DB}\" ] then # Fetch user parameter for not extracted db path and run flock (see next section) DB_PATH = \"not extracted\" else # Set variable to extracted db path fi Since the download is not directly handled by nextflow and paths to the files need to be downloaded, any file or directory must be mounted first to the container. For this reason you have to add the setDockerMount function with the database config as input to the containerOptions parameter: containerOptions \" other container options \" + setDockerMount ( params.steps?.magAttributes?.checkm?.database ) Filesystem locks Multiple jobs of the same process (e.g. GTDB) are able to synchronize the download of a database by using filesystem locks. The download is handled by the concurrentDownload.sh script and should be executed the following way: flock LOCK_FILE concurrentDownload.sh --output = DATABASE \\ --httpsCommand = COMMAND \\ --localCommand = COMMAND \\ --s3FileCommand = COMMAND \\ --s3DirectoryCommand = COMMAND \\ --s5cmdAdditionalParams = S5CMD_PARAMS \\ --link = LINK \\ --expectedMD5SUM = USER_VERIFIED_DATABASE_MD5SUM where * LOCK_FILE is a file that is used for locking. Processes will check if the file is currently locked before trying to download anything. This file should ideally placed in the params.database directory of the specific tool (e.g. !{params.databases}/rgi). DATABASE is the directory that is used for placing the specific database. COMMAND is the command used to download and extract the database and to remove it afterwards. (e.g. \"wget -O data.tar.gz $DOWNLOAD_LINK && tar -xvf data.tar.gz ./card.json && rm data.tar.gz\" for the --httpsCommand flag) USER_VERIFIED_DATABASE_MD5SUM is the MD5SUM of the extracted database that the user should test manually before executing the pipeline. S5CMD_PARAMS allows you to set s5cmd specific parameters. For more information check the s5cmd documentation. LINK is the link that will be used to test if the file is accessible by S3, HTTPS or is available via a local path. USER_VERIFIED_DATABASE_MD5SUM Before a database is downloaded, the script checks the MD5SUM of an already downloaded database against a user specified one. If it does not equal, the script will download the database again. Tests You can test your tool against different database inputs by using the make runDatabaseTest command. You will have to specify multiple databases that are accessible via https, S3, local path etc. Please check github actions file for how to run these tests. Polished Variables Sometimes user input variables must be polished before they can used in our code. Thats why the nextflow config adds a namespace to the params namespace called polished . For example the params.databases variable must end with a slash in order to be used as part of a docker mount. Thats why there is a variable params.polished.databases that should be used instead. Other Magic numbers should not be used. Variable, method, workflow, folder and process names should be written in camelcase.","title":"Guidelines"},{"location":"developer_guidelines/#guidelines","text":"","title":"Guidelines"},{"location":"developer_guidelines/#commit-and-release-guidelines","text":"We are using git-chglog to automatically generate a changelog for the latest released based on our commit messages. Commit messages should follow the following format: feat(scope): feature added in the scope Example: feat(assembly): megahit added feat can be replaced by one of the formats specified in the options sections of the config file (see example below). Scope can for example represent a module, a configuration or a specific document. A new release should be made the following way: Update pipeline version in the nextflow manifest nextflow.config . Create a release on Github. Run git fetch on the master branch to get the latest tag. Run make changelog and paste the output on the Github release section.","title":"Commit and Release Guidelines"},{"location":"developer_guidelines/#versioning","text":"Following semantic versioning , we define the configuration input file and the output folder structure as our public API . Changes to the version numbers reflect updates to the config or the output folders and files. The toolkit consists of many modules that can be used in different combinations and because of this flexibility, we had to come up with a detailed versioning system. We version each module separately, as well as the pipeline itself. Module MAJOR version numbers are updated when a module-specific input parameter is updated or the output folder or file structure is changed. All module version numbers can be retrieved by running the toolkit with the wGetModuleVersion entry point and should be reported on the release page. The module version number is incorporated in the output directory (see output specification ) for easier parsing of the output directory. In the following we give examples when to increment which part of the version identifier: Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible changes, as for example modifying the output structure. A script that was build to parse the output structure must be adapted then. MINOR version when you add functionality in a backward compatible manner. One example is adding an additional tool to the module. PATCH version when you make backwards compatible bug fixes. This is necessary when you for example increment the docker container version number that fixes a bug or increases the speed of the tool. The pipeline specific version number defined in the manifest part of the nextflow.config should be changed if either any module specific version number is incremented or any module-independent parameter (e.g. tempdir ) or output structure is changed.","title":"Versioning"},{"location":"developer_guidelines/#testing","text":"Tests for local use are specified in the scripts folder. These scripts are also used as part of the continuous integration tests. If you want to run these scripts locally, you will have to override the paths to the databases you have downloaded: Examples: bash scripts/test_fullPipeline.sh \" --steps.magAttributes.checkm.database=/vol/spool/checkm --steps.magAttributes.gtdb.database=/vol/spool/gtdb/release202 \" bash scripts/test_fragmentRecruitment.sh \" --steps.fragmentRecruitment.frhit.genomes=test/bins/small/bin.*.fa --steps.fragmentRecruitment.frhit.samples=test/reads/small/reads.tsv \" bash scripts/test_dereplication.sh \" --steps.dereplication.pasolli.input=test/bins/small/attributes.tsv \" bash scripts/test_magAttributes.sh \" --steps.magAttributes.input=test/bins/small/attributes.tsv \"","title":"Testing"},{"location":"developer_guidelines/#nextflow-versions","text":"The toolkit is tested against the lowest and highest Nextflow version number specified in VERSIONS.txt.","title":"Nextflow Versions"},{"location":"developer_guidelines/#modules","text":"Functionality is structured in modules (assembly, binning, dereplication, .etc). Each module can have multiple workflows. Every module follows the output definition specified in the output specification document. The name and the version of the module is specified in the modules section of the nextflow.config file.","title":"Modules"},{"location":"developer_guidelines/#workflows","text":"Worfklow names that can not be used directly and are just meant for internal use should start with an underscore. At least every workflow that can be used by other external workflows should contain a short description of the functionality. Workflow names must start with w .","title":"Workflows"},{"location":"developer_guidelines/#process","text":"Process names should start p . The in- and output of processes should contain a sample and/or a bin and contig id. Custom error strategies that do not follow the strategy defined in nextflow.config, should be documented (see Megahit example).","title":"Process"},{"location":"developer_guidelines/#processes-should-publish-process-specific-files","text":"Processes should publish .command.sh , .command.out , .command.log and .command.err files but never .command.run . In cases where processes process different data but publish it to the same folder these files would be overwritten on every run. For example when Prokka publishes log files of every genome to the same sample directory. For that reason these files need to be renamed, so that their names include a unique id (e.g. bin id). Please output those files to channel with the following entries and connect this channel to the pDumpLogs process that you can import from the utils module: include { pDumpLogs } from ' .. / utils / processes ' ... tuple env ( FILE_ID ), val ( \"${output}\" ), val ( params . LOG_LEVELS . INFO ), file ( \".command.sh\" ), \\ file ( \".command.out\" ), file ( \".command.err\" ), file ( \".command.log\" ), emit : logs Examples can be viewed in the Checkm and Prokka process.","title":"Processes should publish process specific files"},{"location":"developer_guidelines/#logs","text":"Log files should be stored in the user provided logDir directory.","title":"Logs"},{"location":"developer_guidelines/#log-level","text":"Every configuration file must have a logLevel attribute that can have the following values: ALL = 0 All logs are published INFO = 1 Just necessary logs are published These values can be used in the publish dir directive to enable or disable the output of logs. publishDir params . output , mode : \"${params.publishDirMode}\" , saveAs : { filename -> getOutput ( params . runid , \"pasolli/mash/sketch\" , filename ) }, \\ pattern : \"{**.out,**.err, **.sh, **.log}\" , enabled : params . logLevel <= params . LOG_LEVELS . ALL Furthermore the params.LOG_LEVELS.* parameters can be used inside of a process to enable or disable intermediate results for debugging purposes. In cases where the log is send to the pDumpLogs process (see Process section), you can specify the log level as part of the tuple: tuple env(FILE_ID), val(\"${output}\"), val(params.LOG_LEVELS.INFO), file(\".command.sh\"), \\ file(\".command.out\"), file(\".command.err\"), file(\".command.log\"), emit: logs","title":"Log Level"},{"location":"developer_guidelines/#time-limit","text":"Every process must define a time limit which will never be reached on \"normal\" execution. This limit is only useful for errors in the execution environment which could lead to an endless execution of the process. You can use the setTimeLimit helper method to add a user configurable time limit. Example: time Utils.setTimeLimit(params.steps.qc.fastp, params.modules.qc.process.fastp.defaults, params.resources.highmemMedium)","title":"Time Limit"},{"location":"developer_guidelines/#databases","text":"If the same database is downloaded during runtime by multiple processes, it takes up an unnecessary ammount of disc space. One idea is too always use the same place to store these databases. This place should be described in params.databases . If other processes try to use this databases they can look at params.databases on the current machine. If it is present it can be used, if not it should be downloaded. Through this procedure only one copy of each databases is used, which is space-saving. Links to the actual database should contain the database version number or the date of download.","title":"Databases"},{"location":"developer_guidelines/#configuration","text":"Every process should be configurable by providing a parameters string to the tool in the process. Every module should use the following specification in the configuration file: steps : moduleName : parameter : 42 processName : additionalParams : \" --super-flag \" timeLimit : \"AUTO\" Please check the process chapter regarding possible values for the time limit attribute. Additional params can have a string value (like the example above) that is provided to the tool: pProcess { ... shell : \"\"\" supertool !{params.steps.moduleName.processName.parameter} !{params.steps.moduleName.processName.additionalParams} \"\"\" } The value of the additionalParams key can also be a map if multiple tools are used in the same process: steps : moduleName : parameter : 42 processName : additionalParams : toolNameA : \" -c 84 \" toolNameB : \" --super-flag \" parameter fields can hold hardcoded parameters that hold a defined value like a number that should not be a string. One use case of those parameters is that they can be reused for multiple tools. Example: pProcess { ... shell: \"\"\" toolNameA --super-specific-number-flag !{params.steps.moduleName.parameter} toolNameB --similar-flag-to-toolA !{params.steps.moduleName.parameter} \"\"\" }","title":"Configuration"},{"location":"developer_guidelines/#internal-configuration","text":"The _wConfigurePipeline workflow in the main.nf file should be used for setting pipeline parameters that are need for fullfilling the user provided configuration. Example: Lets assume the user enables the plasmid module. In that case it is mandatory that the assembler produces a fastg file independend of the user provided settings of the assembler. In that case the fastg parameter of any assembler will be set to true by the _wConfigurePipeline method.","title":"Internal Configuration"},{"location":"developer_guidelines/#toolkit-docker-images","text":"Dockerfiles of Docker images that are build by toolkit developers can be found in the docker directory. The name of the directory (i.e.: toolkit-python-env in docker/toolkit-python-env ) is used for the docker image name. All images belong to the metagenomics quay.io organisation which is owned by the Computational Metagenomics group in Bielefeld. A docker repository in the metagenomics orginsation must be created by the organisation owner, before the actual image can be build. The version of the image specified in the VERSION file (i.e. docker/toolkit-python-env/VERSION ) is used for the image tag ( metagenomics/toolkit-python-env:VERSION ). An image build is only triggered if the version in the VERSION file is updated on the dev or master branch.","title":"Toolkit Docker Images"},{"location":"developer_guidelines/#wiki","text":"For building the documentation we are using mkdocs in combination with mkdocs-material and a plugin for building static single page html files. The wiki HTML files are uploaded to S3 storage on pull request merge events in the master and dev branch (see Makefile commands using make help ). You can work on these html files locally by running make dev_wiki . But please note that by build the static html file for upload, the navigation might change. You can view the final html file by building the html file (see Makefile make help ).","title":"Wiki"},{"location":"developer_guidelines/#utils","text":"We do not want to duplicate code and thats why we should store methods in the lib/Utils.groovy file. The Utils class can be used in any module.","title":"Utils"},{"location":"developer_guidelines/#database-download","text":"This section explains how a developer is able to implement the database download strategy as explained in the user documentation . Example implementations can be found in the gtdb, checkm or rgi scripts. The first step is to check if the user provides an already extracted database: DB_PATH = \"\" if [ -z \"!{EXTRACTED_DB}\" ] then # Fetch user parameter for not extracted db path and run flock (see next section) DB_PATH = \"not extracted\" else # Set variable to extracted db path fi Since the download is not directly handled by nextflow and paths to the files need to be downloaded, any file or directory must be mounted first to the container. For this reason you have to add the setDockerMount function with the database config as input to the containerOptions parameter: containerOptions \" other container options \" + setDockerMount ( params.steps?.magAttributes?.checkm?.database )","title":"Database Download"},{"location":"developer_guidelines/#filesystem-locks","text":"Multiple jobs of the same process (e.g. GTDB) are able to synchronize the download of a database by using filesystem locks. The download is handled by the concurrentDownload.sh script and should be executed the following way: flock LOCK_FILE concurrentDownload.sh --output = DATABASE \\ --httpsCommand = COMMAND \\ --localCommand = COMMAND \\ --s3FileCommand = COMMAND \\ --s3DirectoryCommand = COMMAND \\ --s5cmdAdditionalParams = S5CMD_PARAMS \\ --link = LINK \\ --expectedMD5SUM = USER_VERIFIED_DATABASE_MD5SUM where * LOCK_FILE is a file that is used for locking. Processes will check if the file is currently locked before trying to download anything. This file should ideally placed in the params.database directory of the specific tool (e.g. !{params.databases}/rgi). DATABASE is the directory that is used for placing the specific database. COMMAND is the command used to download and extract the database and to remove it afterwards. (e.g. \"wget -O data.tar.gz $DOWNLOAD_LINK && tar -xvf data.tar.gz ./card.json && rm data.tar.gz\" for the --httpsCommand flag) USER_VERIFIED_DATABASE_MD5SUM is the MD5SUM of the extracted database that the user should test manually before executing the pipeline. S5CMD_PARAMS allows you to set s5cmd specific parameters. For more information check the s5cmd documentation. LINK is the link that will be used to test if the file is accessible by S3, HTTPS or is available via a local path. USER_VERIFIED_DATABASE_MD5SUM Before a database is downloaded, the script checks the MD5SUM of an already downloaded database against a user specified one. If it does not equal, the script will download the database again.","title":"Filesystem locks"},{"location":"developer_guidelines/#tests","text":"You can test your tool against different database inputs by using the make runDatabaseTest command. You will have to specify multiple databases that are accessible via https, S3, local path etc. Please check github actions file for how to run these tests.","title":"Tests"},{"location":"developer_guidelines/#polished-variables","text":"Sometimes user input variables must be polished before they can used in our code. Thats why the nextflow config adds a namespace to the params namespace called polished . For example the params.databases variable must end with a slash in order to be used as part of a docker mount. Thats why there is a variable params.polished.databases that should be used instead.","title":"Polished Variables"},{"location":"developer_guidelines/#other","text":"Magic numbers should not be used. Variable, method, workflow, folder and process names should be written in camelcase.","title":"Other"},{"location":"module_specification/","text":"Module Specification Assembly Version: 0.2.0 Output: Assembly file names must fulfill the following name pattern: SAMPLENAME_contigs.fa.gz Contig names must be renamed according to the following pattern: SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH where SAMPLEID is the name of the dataset (e.g: SRR234235 ) SEQUENCECOUNTER is the counter of the contig entry in the fasta file (e.g: 2) SEQUENCEHASH are the last 5 characters of an md5sum hash of the fasta entry without the header and newline character. (eg. echo -n \"ACGT\" | md5sum | cut -d ' ' -f 1 | cut -c -5 ) Binning Version: 0.5.0 Output: Binning file names must fulfill the following name pattern: SAMPLENAME_bin.NUMBER.fa Where NUMBER is a unique identifier per SAMPLE. Contig names must be renamed according to the following pattern: SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH MAG=BINNUMBER where SAMPLEID , SEQUENCECOUNTER and SEQUENCEHASH definitions can be inspected in the assembly specification. BINNUMBER is an unique identifier per SAMPLE. MAG Attributes Version: 0.1.0 Output: SAMPLENAME_TOOLNAME_CHUNK.tsv where * TOOLNAME could be for example checkm , gtdb etc. * CHUNK is a random identifier that produces values for one part of all MAGs of a given sample. FORMAT The header line specifies the following columns: BIN_ID SAMPLE BIN_ATTRIBUTE1 BIN_ATTRIBUTE2 ... where * BIN_ID is unique for the samples. * BIN_ATTRIBUTES All column names that are not BIN_ID or SAMPLE can be any property of a MAG, like contamination, completeness etc. Quality Control Version: 0.1.0 Output: SAMPLE_interleaved.qc.fq.gz","title":"Module Specification"},{"location":"module_specification/#module-specification","text":"","title":"Module Specification"},{"location":"module_specification/#assembly","text":"Version: 0.2.0","title":"Assembly"},{"location":"module_specification/#output","text":"Assembly file names must fulfill the following name pattern: SAMPLENAME_contigs.fa.gz Contig names must be renamed according to the following pattern: SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH where SAMPLEID is the name of the dataset (e.g: SRR234235 ) SEQUENCECOUNTER is the counter of the contig entry in the fasta file (e.g: 2) SEQUENCEHASH are the last 5 characters of an md5sum hash of the fasta entry without the header and newline character. (eg. echo -n \"ACGT\" | md5sum | cut -d ' ' -f 1 | cut -c -5 )","title":"Output:"},{"location":"module_specification/#binning","text":"Version: 0.5.0","title":"Binning"},{"location":"module_specification/#output_1","text":"Binning file names must fulfill the following name pattern: SAMPLENAME_bin.NUMBER.fa Where NUMBER is a unique identifier per SAMPLE. Contig names must be renamed according to the following pattern: SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH MAG=BINNUMBER where SAMPLEID , SEQUENCECOUNTER and SEQUENCEHASH definitions can be inspected in the assembly specification. BINNUMBER is an unique identifier per SAMPLE.","title":"Output:"},{"location":"module_specification/#mag-attributes","text":"Version: 0.1.0","title":"MAG Attributes"},{"location":"module_specification/#output_2","text":"SAMPLENAME_TOOLNAME_CHUNK.tsv where * TOOLNAME could be for example checkm , gtdb etc. * CHUNK is a random identifier that produces values for one part of all MAGs of a given sample.","title":"Output:"},{"location":"module_specification/#format","text":"The header line specifies the following columns: BIN_ID SAMPLE BIN_ATTRIBUTE1 BIN_ATTRIBUTE2 ... where * BIN_ID is unique for the samples. * BIN_ATTRIBUTES All column names that are not BIN_ID or SAMPLE can be any property of a MAG, like contamination, completeness etc.","title":"FORMAT"},{"location":"module_specification/#quality-control","text":"Version: 0.1.0","title":"Quality Control"},{"location":"module_specification/#output_3","text":"SAMPLE_interleaved.qc.fq.gz","title":"Output:"},{"location":"pipeline_configuration/","text":"Global parameter settings tempdir : Temporary directory for storing files that are used to collect intermediate files. summary : If true a summary folder is created storing results of all samples combined. output : Output directory for storing pipeline results. If an S3 bucket is specified with the corresponding S3 credentials (See S3 configuration section) then the output is written to S3. runid : The run ID will be part of the output path and allows to distinguish between different pipeline configurations that were used for the same dataset. logDir : A path to a directory which is used to store log files. scratch : The scratch value can be either false or a path on a worker node. If a path is set, then the nextflow process in slurm mode is executed on the provided path. If the standard mode is used, then the parameter is ignored. steps : Steps allows to specify multiple pipeline modules for running the toolkit. We distinguish between two modes. You can either run one tool of the pipeline or the whole pipeline with different configurations. databases : This parameter specifies a place where files are downloaded to. If the slurm profile is used and databases should be downloaded, the path should point to a folder which is not shared between the worker nodes (to reduce I/O on the shared folder resulting in a better performance). If this parameter is provided, the toolkit will create the specified directory. If all your databases have already been extracted beforehand, you can simply omit this parameter. publishDirMode : (optional) Per default results are symlinked to the chosen output directory. This default mode can be changed with this parameter. A useful mode is \"copy\", to copy results instead of just linking them. Other modes to choose from here . skipVersionCheck : The toolkit is regurarly tested against a set of Nextflow versions. Setting the --skipVersionCheck allows you to use the toolkit with Nextflow versions that were not tested. s3SignIn : If your input data (not the databases) is not publicly accessible via S3, then you will have to set the s3SignIn parameter to true . S3 Configuration All module inputs and outputs can be used in conjunction with S3. If you want to set a custom S3 configuration setting (i.e. custom S3 endpoint), you will have to modify the aws client parameters with \" -c \". Example: aws { client { s_3_path_style_access = true maxParallelTransfers = 28 maxErrorRetry = 10 protocol = 'HTTPS' endpoint = 'https://openstack.cebitec.uni-bielefeld.de:8080' signerOverride = 'AWSS3V4SignerType' } } In addition you will have to set a Nextflow Secret with the following keys: nextflow secrets set S3_ACCESS xxxxxxxxx nextflow secrets set S3_SECRET xxxxxxxxx S3_ACCESS corresponds to the aws S3 access key id and S3_SECRET is the aws S3 secret key. If your input data (not the databases) is publicly available then you have to set s3SignIn: to false in your config file. Please note that for using databases you have to provide an additional aws credentials file (see database section). Configuration of input parameters of the full pipeline mode Paired End Input The input should be a path to a tsv file containing a sample id, as well as a path to the left and right read. Example: input: paired: path: \"test_data/fullPipeline/reads_split.tsv\" Nanopore Input For Nanopore data a seperate input file should be specified. input: ont: path: \"test_data/fullPipeline/ont.tsv\" Generic SRA The toolkit is able to fetch fastq files based on SRA run accession ids from the NCBI or from a mirror based on S3: input: SRA: pattern: ont: \".+[^(_1|_2)].+$\" illumina: \".+(_1|_2).+$\" S3: path: test_data/SRA/samples.tsv bucket: \"s3://ftp.era.ebi.ac.uk\" prefix: \"/vol1/fastq/\" watch: false patternONT: \".+[^(_1|_2)].+$\" patternIllumina: \".+(_1|_2).+$\" where: * path is the path to a file containing a column with ACCESSION as header. The ACCESSION column contains either SRA run or study accessions. bucket is the S3 Bucket hosting the data. prefix is the path to the actual SRA datasets. watch if true, the file specified with the path attribute is watched and every time a new SRA run id is appended, the pipeline is triggered. The pipeline will never finish in this mode. Please note that watch currently only works if only one input type is specified (e.g \"ont\" or \"paired\" ...) patternONT and patternIllumina are patterns that are applied on the specified mirror in order to select the correct input files. NCBI SRA With the following mode SRA datasets can directly be fetched from SRA. input: SRA: pattern: ont: \".+[^(_1|_2)].+$\" illumina: \".+(_1|_2).+$\" NCBI: path: test_data/SRA/samples.tsv Database input configuration Whenever a database field can be specified as part of the tool configuration (such as in gtdb or checkm), you are able to provide different methods to fetch the database. In all settings, please make sure that the file has the same ending (e.g. .zip, .tar.gz) as specified in the corresponding tool section. In addition, as database names are used to name results with which they were created, said database names should contain the respective database number or date of creation. With this every result can be linked to one exact database version to clarify results. Except for the extractedDBPath parameter, all other input types (https, s3,...) will download the database to the folder specified in the database parameter. Extracted Database Path If you have already downloaded and extracted the database, you can specify the path using the extractedDBPath parameter. This setting is available in standard and slurm mode. In slurm mode the path can point to a db on the worker node. Example: database: extractedDBPath: /vol/spool/gtdb/release202 HTTPS Download The toolkit is able to download and extract the database, as long as the file ending equals the one specified in the corresponding tool section (.zip, tar.gz, tar.zst) This setting is available in standard and slurm mode. Example: database: download: source: 'https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdb.tar.gz' md5sum: 77180f6a02769e7eec6b8c22d3614d2e Local File Path This setting allows you to reuse an already downloaded database. Example: database: download: source: '/vol/spool/gtdb.tar.gz' md5sum: 77180f6a02769e7eec6b8c22d3614d2e S3 Download You can specify an S3 link and configure the S3 call via the s5cmd.params parameter. The s5cmd.params` parameter allows you to set any setting available of the s5cmd commandline tool. If you need credentials to access your databases, you can set them via the Nextflow secrets mechanism. The correct key name for the access and secret key can be found in the corresponding database section. In the following example the compressed file will be downloaded and extracted. Example for publicly available compressed database: database: download: source: 's3://databases/gtdb.tar.gz' md5sum: 77180f6a02769e7eec6b8c22d3614d2e s5cmd: params: '--retry-count 30 --no-sign-request --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080' If your database is already extracted and available via S3, you can specify the S3 link using a wildcard as in the next example. database: download: source: 's3://databases/gtdb/*' md5sum: 77180f6a02769e7eec6b8c22d3614d2e s5cmd: params: '--retry-count 30 --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080' Updating Database MD5SUMs The md5sum is computed over all md5sums of all files of the extracted database. If you need to update the md5sum because you updated your database you have to download the database and run the following command find /path/to/db -type f -exec md5sum {} \\; | sort | cut -d ' ' -f 1 | md5sum | cut -d ' ' -f 1 Database Download strategy The toolkit allows to download databases on multiple nodes and tries to synchronize the download process between multiple jobs on a node. However not all possible combinations of profiles and download types are reasonable. PROFILE Download to Shared NFS Download to worker scratch dir Reuse extracted directory STANDARD :material-check: :material-close: :material-check: SLURM :material-check-all: :material-check: :material-check: On scratch and nfs dir Optional configuration of computational resources used for pipeline runs The toolkit uses the following machine types (flavors) for running tools. All flavors can be optionally adjusted by modifying the cpus and memory (in GB) parameters. If for example the largest flavor is not available in the infrastructure, cpus and memory parameters can be modified to fit the highmemMedium flavor. If larger flavors are available, it makes especially sense to increase the cpus and memory values of the large flavor to speed up for example assembly and read mapping. Example Configuration: resources: highmemLarge: cpus: 28 memory: 230 highmemMedium: cpus: 14 memory: 113 large: cpus: 28 memory: 58 medium: cpus: 14 memory: 29 small: cpus: 7 memory: 14 tiny: cpus: 1 memory: 1 Additional flavors can be defined that can be used by methods that dynamically compute resources on tool error (see assembly module section). Example: resources: xlarge: cpus: 56 memory: 512 highmemLarge: cpus: 28 memory: 230 highmemMedium: cpus: 14 memory: 113 large: cpus: 28 memory: 58 medium: cpus: 14 memory: 29 small: cpus: 7 memory: 14 tiny: cpus: 1 memory: 1 The full pipeline mode is able to predict the memory consumption of some assemblers (see assembly module section). Fragment Recruitment for unmapped reads Configuration Reads that could not be mapped back to a MAG can be used for fragment recruitment. A list of genomes can be provided in the fragmentRecruitment part. Matched reference genomes are included in all other parts of the remaining pipeline. Look out for their specific headers to differentiate results based on real assembled genomes and the reference genomes.","title":"Global Pipeline Configuration Settings"},{"location":"pipeline_configuration/#global-parameter-settings","text":"tempdir : Temporary directory for storing files that are used to collect intermediate files. summary : If true a summary folder is created storing results of all samples combined. output : Output directory for storing pipeline results. If an S3 bucket is specified with the corresponding S3 credentials (See S3 configuration section) then the output is written to S3. runid : The run ID will be part of the output path and allows to distinguish between different pipeline configurations that were used for the same dataset. logDir : A path to a directory which is used to store log files. scratch : The scratch value can be either false or a path on a worker node. If a path is set, then the nextflow process in slurm mode is executed on the provided path. If the standard mode is used, then the parameter is ignored. steps : Steps allows to specify multiple pipeline modules for running the toolkit. We distinguish between two modes. You can either run one tool of the pipeline or the whole pipeline with different configurations. databases : This parameter specifies a place where files are downloaded to. If the slurm profile is used and databases should be downloaded, the path should point to a folder which is not shared between the worker nodes (to reduce I/O on the shared folder resulting in a better performance). If this parameter is provided, the toolkit will create the specified directory. If all your databases have already been extracted beforehand, you can simply omit this parameter. publishDirMode : (optional) Per default results are symlinked to the chosen output directory. This default mode can be changed with this parameter. A useful mode is \"copy\", to copy results instead of just linking them. Other modes to choose from here . skipVersionCheck : The toolkit is regurarly tested against a set of Nextflow versions. Setting the --skipVersionCheck allows you to use the toolkit with Nextflow versions that were not tested. s3SignIn : If your input data (not the databases) is not publicly accessible via S3, then you will have to set the s3SignIn parameter to true .","title":"Global parameter settings"},{"location":"pipeline_configuration/#s3-configuration","text":"All module inputs and outputs can be used in conjunction with S3. If you want to set a custom S3 configuration setting (i.e. custom S3 endpoint), you will have to modify the aws client parameters with \" -c \". Example: aws { client { s_3_path_style_access = true maxParallelTransfers = 28 maxErrorRetry = 10 protocol = 'HTTPS' endpoint = 'https://openstack.cebitec.uni-bielefeld.de:8080' signerOverride = 'AWSS3V4SignerType' } } In addition you will have to set a Nextflow Secret with the following keys: nextflow secrets set S3_ACCESS xxxxxxxxx nextflow secrets set S3_SECRET xxxxxxxxx S3_ACCESS corresponds to the aws S3 access key id and S3_SECRET is the aws S3 secret key. If your input data (not the databases) is publicly available then you have to set s3SignIn: to false in your config file. Please note that for using databases you have to provide an additional aws credentials file (see database section).","title":"S3 Configuration"},{"location":"pipeline_configuration/#configuration-of-input-parameters-of-the-full-pipeline-mode","text":"","title":"Configuration of input parameters of the full pipeline mode"},{"location":"pipeline_configuration/#paired-end-input","text":"The input should be a path to a tsv file containing a sample id, as well as a path to the left and right read. Example: input: paired: path: \"test_data/fullPipeline/reads_split.tsv\"","title":"Paired End Input"},{"location":"pipeline_configuration/#nanopore-input","text":"For Nanopore data a seperate input file should be specified. input: ont: path: \"test_data/fullPipeline/ont.tsv\"","title":"Nanopore Input"},{"location":"pipeline_configuration/#generic-sra","text":"The toolkit is able to fetch fastq files based on SRA run accession ids from the NCBI or from a mirror based on S3: input: SRA: pattern: ont: \".+[^(_1|_2)].+$\" illumina: \".+(_1|_2).+$\" S3: path: test_data/SRA/samples.tsv bucket: \"s3://ftp.era.ebi.ac.uk\" prefix: \"/vol1/fastq/\" watch: false patternONT: \".+[^(_1|_2)].+$\" patternIllumina: \".+(_1|_2).+$\" where: * path is the path to a file containing a column with ACCESSION as header. The ACCESSION column contains either SRA run or study accessions. bucket is the S3 Bucket hosting the data. prefix is the path to the actual SRA datasets. watch if true, the file specified with the path attribute is watched and every time a new SRA run id is appended, the pipeline is triggered. The pipeline will never finish in this mode. Please note that watch currently only works if only one input type is specified (e.g \"ont\" or \"paired\" ...) patternONT and patternIllumina are patterns that are applied on the specified mirror in order to select the correct input files.","title":"Generic SRA"},{"location":"pipeline_configuration/#ncbi-sra","text":"With the following mode SRA datasets can directly be fetched from SRA. input: SRA: pattern: ont: \".+[^(_1|_2)].+$\" illumina: \".+(_1|_2).+$\" NCBI: path: test_data/SRA/samples.tsv","title":"NCBI SRA"},{"location":"pipeline_configuration/#database-input-configuration","text":"Whenever a database field can be specified as part of the tool configuration (such as in gtdb or checkm), you are able to provide different methods to fetch the database. In all settings, please make sure that the file has the same ending (e.g. .zip, .tar.gz) as specified in the corresponding tool section. In addition, as database names are used to name results with which they were created, said database names should contain the respective database number or date of creation. With this every result can be linked to one exact database version to clarify results. Except for the extractedDBPath parameter, all other input types (https, s3,...) will download the database to the folder specified in the database parameter.","title":"Database input configuration"},{"location":"pipeline_configuration/#extracted-database-path","text":"If you have already downloaded and extracted the database, you can specify the path using the extractedDBPath parameter. This setting is available in standard and slurm mode. In slurm mode the path can point to a db on the worker node. Example: database: extractedDBPath: /vol/spool/gtdb/release202","title":"Extracted Database Path"},{"location":"pipeline_configuration/#https-download","text":"The toolkit is able to download and extract the database, as long as the file ending equals the one specified in the corresponding tool section (.zip, tar.gz, tar.zst) This setting is available in standard and slurm mode. Example: database: download: source: 'https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdb.tar.gz' md5sum: 77180f6a02769e7eec6b8c22d3614d2e","title":"HTTPS Download"},{"location":"pipeline_configuration/#local-file-path","text":"This setting allows you to reuse an already downloaded database. Example: database: download: source: '/vol/spool/gtdb.tar.gz' md5sum: 77180f6a02769e7eec6b8c22d3614d2e","title":"Local File Path"},{"location":"pipeline_configuration/#s3-download","text":"You can specify an S3 link and configure the S3 call via the s5cmd.params parameter. The s5cmd.params` parameter allows you to set any setting available of the s5cmd commandline tool. If you need credentials to access your databases, you can set them via the Nextflow secrets mechanism. The correct key name for the access and secret key can be found in the corresponding database section. In the following example the compressed file will be downloaded and extracted. Example for publicly available compressed database: database: download: source: 's3://databases/gtdb.tar.gz' md5sum: 77180f6a02769e7eec6b8c22d3614d2e s5cmd: params: '--retry-count 30 --no-sign-request --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080' If your database is already extracted and available via S3, you can specify the S3 link using a wildcard as in the next example. database: download: source: 's3://databases/gtdb/*' md5sum: 77180f6a02769e7eec6b8c22d3614d2e s5cmd: params: '--retry-count 30 --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080'","title":"S3 Download"},{"location":"pipeline_configuration/#updating-database-md5sums","text":"The md5sum is computed over all md5sums of all files of the extracted database. If you need to update the md5sum because you updated your database you have to download the database and run the following command find /path/to/db -type f -exec md5sum {} \\; | sort | cut -d ' ' -f 1 | md5sum | cut -d ' ' -f 1","title":"Updating Database MD5SUMs"},{"location":"pipeline_configuration/#database-download-strategy","text":"The toolkit allows to download databases on multiple nodes and tries to synchronize the download process between multiple jobs on a node. However not all possible combinations of profiles and download types are reasonable. PROFILE Download to Shared NFS Download to worker scratch dir Reuse extracted directory STANDARD :material-check: :material-close: :material-check: SLURM :material-check-all: :material-check: :material-check: On scratch and nfs dir","title":"Database Download strategy"},{"location":"pipeline_configuration/#optional-configuration-of-computational-resources-used-for-pipeline-runs","text":"The toolkit uses the following machine types (flavors) for running tools. All flavors can be optionally adjusted by modifying the cpus and memory (in GB) parameters. If for example the largest flavor is not available in the infrastructure, cpus and memory parameters can be modified to fit the highmemMedium flavor. If larger flavors are available, it makes especially sense to increase the cpus and memory values of the large flavor to speed up for example assembly and read mapping. Example Configuration: resources: highmemLarge: cpus: 28 memory: 230 highmemMedium: cpus: 14 memory: 113 large: cpus: 28 memory: 58 medium: cpus: 14 memory: 29 small: cpus: 7 memory: 14 tiny: cpus: 1 memory: 1 Additional flavors can be defined that can be used by methods that dynamically compute resources on tool error (see assembly module section). Example: resources: xlarge: cpus: 56 memory: 512 highmemLarge: cpus: 28 memory: 230 highmemMedium: cpus: 14 memory: 113 large: cpus: 28 memory: 58 medium: cpus: 14 memory: 29 small: cpus: 7 memory: 14 tiny: cpus: 1 memory: 1 The full pipeline mode is able to predict the memory consumption of some assemblers (see assembly module section).","title":"Optional configuration of computational resources used for pipeline runs"},{"location":"pipeline_configuration/#fragment-recruitment-for-unmapped-reads-configuration","text":"Reads that could not be mapped back to a MAG can be used for fragment recruitment. A list of genomes can be provided in the fragmentRecruitment part. Matched reference genomes are included in all other parts of the remaining pipeline. Look out for their specific headers to differentiate results based on real assembled genomes and the reference genomes.","title":"Fragment Recruitment for unmapped reads Configuration"},{"location":"pipeline_specification/","text":"Pipeline Specification Output and best practice Motivation The output section is a collection of best practices for storing results of the meta-omics-toolkit output. The definitions are motivated by the fact that the pipeline will be continuously updated and results of different pipeline versions and modes must be differentiated. The idea is to run the pipeline on results of previous runs. Rules for dataset output Outputs are produced by using the publish dir directive. DATASET_ID/RUN_ID/MODULE/VERSION/TOOL/ where * DATASET_ID specifies the ID of a dataset such as the SRA run ID. * RUN_ID specifies one possible run of the full or partial pipeline. The RUN_ID identifier can be any user provided identifier to keep track of multiple pipeline runs. * MODULE specifies the name of the pipeline module (e.g. binning). * VERSION specifies the module version number which follows semantic versioning (1.2.0). * TOOL specifies the name of the tool that is executed as part of the module (e.g megahit of the assembly module). It is suggested that a RUN_ID output should never contain multiple versions of the same module. E.g.: DATASET_ID/1/Binning/1.2.0/metabat and DATASET_ID/1/Binning/1.3.0/metabat . If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) alreads contains the output of an older version of run (B), then a new RUN_ID folder must be created. If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) does not contain the output of an older version of run (B), then the existing RUN_ID folder must be reused . Run Versioning Every dataset must contain a TOOL folder called config . The config folder contains descriptions of the parameters and the version used for the specific pipeline run. Examples Example 1: We assume that the following folder already exists: /SRA1/1/ASSEMBLY/1.2/MEGAHIT If the MODULE output does not contain a BINNING output then the existing RUN folder must be reused: /SRA1/1/ASSEMBLY/1.2/MEGAHIT /SRA1/1/BINNING/0.3/METABAT Example 2: We assume that the following folders already exists: /SRA1/1/ASSEMBLY/1.2/MEGAHIT /SRA1/1/BINNING/0.3/METABAT If the MODULE output does contain a BINNING output then a new RUN folder must be created: /SRA1/1/ASSEMBLY/1.2/MEGAHIT /SRA1/1/BINNING/0.3/METABAT /SRA1/2/BINNING/0.4/METABAT Rules for aggregated output Aggregates outputs in the same way as dataset outputs are produced, by using the publish dir directive with the only difference that no sample identifier are used and the path starts with AGGREGATED . Example: AGGREGATED/RUN_ID/MODULE/VERSION/TOOL/","title":"Pipeline Specification"},{"location":"pipeline_specification/#pipeline-specification","text":"","title":"Pipeline Specification"},{"location":"pipeline_specification/#output-and-best-practice","text":"","title":"Output and best practice"},{"location":"pipeline_specification/#motivation","text":"The output section is a collection of best practices for storing results of the meta-omics-toolkit output. The definitions are motivated by the fact that the pipeline will be continuously updated and results of different pipeline versions and modes must be differentiated. The idea is to run the pipeline on results of previous runs.","title":"Motivation"},{"location":"pipeline_specification/#rules-for-dataset-output","text":"Outputs are produced by using the publish dir directive. DATASET_ID/RUN_ID/MODULE/VERSION/TOOL/ where * DATASET_ID specifies the ID of a dataset such as the SRA run ID. * RUN_ID specifies one possible run of the full or partial pipeline. The RUN_ID identifier can be any user provided identifier to keep track of multiple pipeline runs. * MODULE specifies the name of the pipeline module (e.g. binning). * VERSION specifies the module version number which follows semantic versioning (1.2.0). * TOOL specifies the name of the tool that is executed as part of the module (e.g megahit of the assembly module). It is suggested that a RUN_ID output should never contain multiple versions of the same module. E.g.: DATASET_ID/1/Binning/1.2.0/metabat and DATASET_ID/1/Binning/1.3.0/metabat . If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) alreads contains the output of an older version of run (B), then a new RUN_ID folder must be created. If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) does not contain the output of an older version of run (B), then the existing RUN_ID folder must be reused .","title":"Rules for dataset output"},{"location":"pipeline_specification/#run-versioning","text":"Every dataset must contain a TOOL folder called config . The config folder contains descriptions of the parameters and the version used for the specific pipeline run.","title":"Run Versioning"},{"location":"pipeline_specification/#examples","text":"","title":"Examples"},{"location":"pipeline_specification/#example-1","text":"We assume that the following folder already exists: /SRA1/1/ASSEMBLY/1.2/MEGAHIT If the MODULE output does not contain a BINNING output then the existing RUN folder must be reused: /SRA1/1/ASSEMBLY/1.2/MEGAHIT /SRA1/1/BINNING/0.3/METABAT","title":"Example 1:"},{"location":"pipeline_specification/#example-2","text":"We assume that the following folders already exists: /SRA1/1/ASSEMBLY/1.2/MEGAHIT /SRA1/1/BINNING/0.3/METABAT If the MODULE output does contain a BINNING output then a new RUN folder must be created: /SRA1/1/ASSEMBLY/1.2/MEGAHIT /SRA1/1/BINNING/0.3/METABAT /SRA1/2/BINNING/0.4/METABAT","title":"Example 2:"},{"location":"pipeline_specification/#rules-for-aggregated-output","text":"Aggregates outputs in the same way as dataset outputs are produced, by using the publish dir directive with the only difference that no sample identifier are used and the path starts with AGGREGATED . Example: AGGREGATED/RUN_ID/MODULE/VERSION/TOOL/","title":"Rules for aggregated output"},{"location":"modules/annotation/","text":"Annotation The annotation module is able to predict genes and annotate those based on Prokka and a set of user provided databases. A user can add additional formatted databases as part of the configuration by adding a key (Example: kegg ) with a possible download strategy. See database section for possible download strategies. In addition, the resistance gene identifier is executed by default. Input Command Configuration File TSV Table -entry wAnnotateLocal -params-file example_params/annotation.yml Databases MMseqs2 MMseqs2 needs a combination of different data, index and dbtype files as \"one\" database, be it in- or output. See MMseqs2 database for more information. As multiple and in most cases, big files are used, tar and zstd are utilized to compress and transport files. Input databases have to be compressed by these and need to end with .tar.zst . Naming inside an archive is irrelevant, as databases are picked automatically. Multiple databases per one archive are not supported, one archive, one database. If the database also includes a taxonomy as described here , it can also be used for taxonomic classifications with MMseqs2 - Taxonomy. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_db_ACCESS XXXXXXX nextflow secrets set S3_db_SECRET XXXXXXX where db is the name of the database that you use in your config file. Example: .... vfdb: params: ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3' database: download: source: s3://databases/vfdb_full_2022_07_29.tar.zst md5sum: 7e32aaed112d6e056fb8764b637bf49e s5cmd: params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" .... Based on these settings, you would set the following secret: nextflow secrets set S3_vfdb_ACCESS XXXXXXX nextflow secrets set S3_vfdb_SECRET XXXXXXX KEGGFromBlast KeGGFromBlast is only executed if genes are searched against a KEGG database. There must be a kegg identifier (see example configuration file) in the annotation section. KeGGFromBlast needs a kegg database as input which must be a tar.gz file. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_kegg_ACCESS XXXXXXX nextflow secrets set S3_kegg_SECRET XXXXXXX MMSeqs Taxonomy If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_TAX_db_ACCESS XXXXXXX nextflow secrets set S3_TAX_db_SECRET XXXXXXX where db is the name of the database that you use in your config file. Example: .... mmseqs2_taxonomy: gtdb: params: ' --orf-filter-s 1 -e 1e-15' ramMode: false database: download: source: s3://databases/gtdb_r214_1_mmseqs.tar.gz md5sum: 3c8f12c5c2dc55841a14dd30a0a4c718 s5cmd: params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" .... Based on these settings, you would set the following secrets: nextflow secrets set S3_TAX_gtdb_ACCESS XXXXXXX nextflow secrets set S3_TAX_gtdb_SECRET XXXXXXX RGI RGI needs a CARD database which can be fetched via this link: https://card.mcmaster.ca/latest/data. The compressed database must be a tar.bz2 file. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_rgi_ACCESS XXXXXXX nextflow secrets set S3_rgi_SECRET XXXXXXX Output MMseqs2 Calculated significant matches of a nucleotide/protein query which was compared against a user provided set of databases. MMseqs2 - Taxonomy By identifying homologous through searches against a provided MMseqs2 taxonomy-database, MMseqs2 can compute the lowest common ancestor. This lowest common ancestor is a robust taxonomic label for unknown sequences. These labels are presented in form of an *.taxonomy.tsv file, a *.krakenStyleTaxonomy.out formatted in accordance to the KRAKEN tool outputs and an interactive KRONA plot in form of a html website *.krona.html . Prokka Prokka computes *.err , *.faa , *.ffn , *.fna , *.fsa , *.gbk , *.gff , *.sqn , *.tbl , *.tbl for every bin. *.gbk and *.sqn are skipped per default, since tbl2asn runs for quite a while! If you need those files generated by prokka, include: --tbl2asn in the prokka parameters to enable it. Details of all files can be read on the Prokka page. In addition, it also computes a summary tsv file which adheres to the magAttributes specification. KEGGFromBlast Result *.tsv file filled with KEGG information (like modules, KO's, ...) which could be linked to the input hits. Resistance Gene Identifier (rgi) The *rgi.tsv files contain the found CARD genes.","title":"Annotation"},{"location":"modules/annotation/#annotation","text":"The annotation module is able to predict genes and annotate those based on Prokka and a set of user provided databases. A user can add additional formatted databases as part of the configuration by adding a key (Example: kegg ) with a possible download strategy. See database section for possible download strategies. In addition, the resistance gene identifier is executed by default.","title":"Annotation"},{"location":"modules/annotation/#input","text":"Command Configuration File TSV Table -entry wAnnotateLocal -params-file example_params/annotation.yml","title":"Input"},{"location":"modules/annotation/#databases","text":"","title":"Databases"},{"location":"modules/annotation/#mmseqs2","text":"MMseqs2 needs a combination of different data, index and dbtype files as \"one\" database, be it in- or output. See MMseqs2 database for more information. As multiple and in most cases, big files are used, tar and zstd are utilized to compress and transport files. Input databases have to be compressed by these and need to end with .tar.zst . Naming inside an archive is irrelevant, as databases are picked automatically. Multiple databases per one archive are not supported, one archive, one database. If the database also includes a taxonomy as described here , it can also be used for taxonomic classifications with MMseqs2 - Taxonomy. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_db_ACCESS XXXXXXX nextflow secrets set S3_db_SECRET XXXXXXX where db is the name of the database that you use in your config file. Example: .... vfdb: params: ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3' database: download: source: s3://databases/vfdb_full_2022_07_29.tar.zst md5sum: 7e32aaed112d6e056fb8764b637bf49e s5cmd: params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" .... Based on these settings, you would set the following secret: nextflow secrets set S3_vfdb_ACCESS XXXXXXX nextflow secrets set S3_vfdb_SECRET XXXXXXX","title":"MMseqs2"},{"location":"modules/annotation/#keggfromblast","text":"KeGGFromBlast is only executed if genes are searched against a KEGG database. There must be a kegg identifier (see example configuration file) in the annotation section. KeGGFromBlast needs a kegg database as input which must be a tar.gz file. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_kegg_ACCESS XXXXXXX nextflow secrets set S3_kegg_SECRET XXXXXXX","title":"KEGGFromBlast"},{"location":"modules/annotation/#mmseqs-taxonomy","text":"If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_TAX_db_ACCESS XXXXXXX nextflow secrets set S3_TAX_db_SECRET XXXXXXX where db is the name of the database that you use in your config file. Example: .... mmseqs2_taxonomy: gtdb: params: ' --orf-filter-s 1 -e 1e-15' ramMode: false database: download: source: s3://databases/gtdb_r214_1_mmseqs.tar.gz md5sum: 3c8f12c5c2dc55841a14dd30a0a4c718 s5cmd: params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" .... Based on these settings, you would set the following secrets: nextflow secrets set S3_TAX_gtdb_ACCESS XXXXXXX nextflow secrets set S3_TAX_gtdb_SECRET XXXXXXX","title":"MMSeqs Taxonomy"},{"location":"modules/annotation/#rgi","text":"RGI needs a CARD database which can be fetched via this link: https://card.mcmaster.ca/latest/data. The compressed database must be a tar.bz2 file. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_rgi_ACCESS XXXXXXX nextflow secrets set S3_rgi_SECRET XXXXXXX","title":"RGI"},{"location":"modules/annotation/#output","text":"","title":"Output"},{"location":"modules/annotation/#mmseqs2_1","text":"Calculated significant matches of a nucleotide/protein query which was compared against a user provided set of databases.","title":"MMseqs2"},{"location":"modules/annotation/#mmseqs2-taxonomy","text":"By identifying homologous through searches against a provided MMseqs2 taxonomy-database, MMseqs2 can compute the lowest common ancestor. This lowest common ancestor is a robust taxonomic label for unknown sequences. These labels are presented in form of an *.taxonomy.tsv file, a *.krakenStyleTaxonomy.out formatted in accordance to the KRAKEN tool outputs and an interactive KRONA plot in form of a html website *.krona.html .","title":"MMseqs2 - Taxonomy"},{"location":"modules/annotation/#prokka","text":"Prokka computes *.err , *.faa , *.ffn , *.fna , *.fsa , *.gbk , *.gff , *.sqn , *.tbl , *.tbl for every bin. *.gbk and *.sqn are skipped per default, since tbl2asn runs for quite a while! If you need those files generated by prokka, include: --tbl2asn in the prokka parameters to enable it. Details of all files can be read on the Prokka page. In addition, it also computes a summary tsv file which adheres to the magAttributes specification.","title":"Prokka"},{"location":"modules/annotation/#keggfromblast_1","text":"Result *.tsv file filled with KEGG information (like modules, KO's, ...) which could be linked to the input hits.","title":"KEGGFromBlast"},{"location":"modules/annotation/#resistance-gene-identifier-rgi","text":"The *rgi.tsv files contain the found CARD genes.","title":"Resistance Gene Identifier (rgi)"},{"location":"modules/assembly/","text":"Assembly Input Command for short read data with optional single end reads Command for long read data Megahit Configuration File Metaspades Configuration File MetaFlye Configuration File TSV Table Short Read TSV Table Nanopore -entry wShortReadAssembly -params-file example_params/assembly.yml -entry wOntAssembly -params-file example_params/assemblyONT.yml Output The output is a gzipped fasta file containing contigs. Megahit Error Handling On error with exit codes ([-9, 137, 247]) (e.g. due to memory restrictions), the tool is executed again with higher cpu and memory values. The memory and cpu values are in case of a retry selected based on the flavor with the next higher memory value. The highest possible cpu/memory value is restricted by the highest cpu/memory value of all flavors defined in the resource section (see global configuration section). Peak memory usage prediction Memory consumption of an assembler varies based on diversity. We trained a machine learning model on kmer frequencies and the nonpareil diversity index in order to be able to predict the memory peak consumption of megahit in our full pipeline mode. The required resources in order to run the assembler are thereby fitted to the resources that are actually needed for a specific dataset. If this mode is enabled then Nonpareil and kmc that are part of the quality control module are automatically executed before the assembler run. Please note that this mode is only tested for Megahit with default parameters and the meta-sensitive mode ( --presets meta-sensitive ). resources: RAM: mode: MODE predictMinLabel: LABEL where * MODE can be either 'PREDICT' for predicting memory usage or 'DEFAULT' for using a default flavor defined in the resources section. * LABEL is the flavor that will be used if the predicted RAM is below the memory value defined as part of the LABEL flavor. It can also be set to AUTO to always use the predicted flavor.","title":"Assembly"},{"location":"modules/assembly/#assembly","text":"","title":"Assembly"},{"location":"modules/assembly/#input","text":"Command for short read data with optional single end reads Command for long read data Megahit Configuration File Metaspades Configuration File MetaFlye Configuration File TSV Table Short Read TSV Table Nanopore -entry wShortReadAssembly -params-file example_params/assembly.yml -entry wOntAssembly -params-file example_params/assemblyONT.yml","title":"Input"},{"location":"modules/assembly/#output","text":"The output is a gzipped fasta file containing contigs.","title":"Output"},{"location":"modules/assembly/#megahit","text":"","title":"Megahit"},{"location":"modules/assembly/#error-handling","text":"On error with exit codes ([-9, 137, 247]) (e.g. due to memory restrictions), the tool is executed again with higher cpu and memory values. The memory and cpu values are in case of a retry selected based on the flavor with the next higher memory value. The highest possible cpu/memory value is restricted by the highest cpu/memory value of all flavors defined in the resource section (see global configuration section).","title":"Error Handling"},{"location":"modules/assembly/#peak-memory-usage-prediction","text":"Memory consumption of an assembler varies based on diversity. We trained a machine learning model on kmer frequencies and the nonpareil diversity index in order to be able to predict the memory peak consumption of megahit in our full pipeline mode. The required resources in order to run the assembler are thereby fitted to the resources that are actually needed for a specific dataset. If this mode is enabled then Nonpareil and kmc that are part of the quality control module are automatically executed before the assembler run. Please note that this mode is only tested for Megahit with default parameters and the meta-sensitive mode ( --presets meta-sensitive ). resources: RAM: mode: MODE predictMinLabel: LABEL where * MODE can be either 'PREDICT' for predicting memory usage or 'DEFAULT' for using a default flavor defined in the resources section. * LABEL is the flavor that will be used if the predicted RAM is below the memory value defined as part of the LABEL flavor. It can also be set to AUTO to always use the predicted flavor.","title":"Peak memory usage prediction"},{"location":"modules/cooccurrence/","text":"Cooccurrence The Cooccurrence module builds a cooccurrence network where each node is a MAG and every edge represents an association between them. The network can be inferred based on correlation or inverse covariance estimation by SPIEC-EASI. SPIEC-EASI is executed multiple times based on different parameter settings in order to find the most stable network. In addition, it is possible to compute multiple metrics for every edge based on genome-scale metabolic models and the SMETANA metrics. -entry wCooccurrence -params-file example_params/cooccurrence.yml Input Command Configuration File for Cooccurrence TSV Table GTDB TSV Table Configuration File for analyzing edges in Cooccurrence Graph GTDB TSV for analyzing Edges Model TSV for computing Metabolomics Metrics on Edges -entry wCooccurrence -params-file example_params/cooccurrence.yml Contains abundance values of mags per sample. GTDB assignment of all samples that were produced by magAttributes module. GTDB assignment of all samples that were produced by the magAttributes module. The following parameters can be configured: metabolicEdgeBatches: Batches of edges that are provided as input to SMETANA. metabolicEdgeReplicates: Number of replicates per edge that should be computed. Output output_raw.graphml: Cooccurrence network unfiltered in graphml format output.graphml: Filtered cooccurrence network in graphml format edges_index.tsv: Edges of the graph edgeAttributes.tsv: Edge attributes of the graph containing metrics computed by SMETANA. SPIEC-EASI stability.txt: Network stability estimation","title":"Cooccurrence"},{"location":"modules/cooccurrence/#cooccurrence","text":"The Cooccurrence module builds a cooccurrence network where each node is a MAG and every edge represents an association between them. The network can be inferred based on correlation or inverse covariance estimation by SPIEC-EASI. SPIEC-EASI is executed multiple times based on different parameter settings in order to find the most stable network. In addition, it is possible to compute multiple metrics for every edge based on genome-scale metabolic models and the SMETANA metrics. -entry wCooccurrence -params-file example_params/cooccurrence.yml","title":"Cooccurrence"},{"location":"modules/cooccurrence/#input","text":"Command Configuration File for Cooccurrence TSV Table GTDB TSV Table Configuration File for analyzing edges in Cooccurrence Graph GTDB TSV for analyzing Edges Model TSV for computing Metabolomics Metrics on Edges -entry wCooccurrence -params-file example_params/cooccurrence.yml Contains abundance values of mags per sample. GTDB assignment of all samples that were produced by magAttributes module. GTDB assignment of all samples that were produced by the magAttributes module. The following parameters can be configured: metabolicEdgeBatches: Batches of edges that are provided as input to SMETANA. metabolicEdgeReplicates: Number of replicates per edge that should be computed.","title":"Input"},{"location":"modules/cooccurrence/#output","text":"output_raw.graphml: Cooccurrence network unfiltered in graphml format output.graphml: Filtered cooccurrence network in graphml format edges_index.tsv: Edges of the graph edgeAttributes.tsv: Edge attributes of the graph containing metrics computed by SMETANA.","title":"Output"},{"location":"modules/cooccurrence/#spiec-easi","text":"stability.txt: Network stability estimation","title":"SPIEC-EASI"},{"location":"modules/dereplication/","text":"Dereplication Input Command Configuration File TSV Table -entry wDereplication -params-file example_params/dereplication.yml Must include the columns DATASET , BIN_ID , PATH , COMPLETENESS , CONTAMINATION , COVERAGE , N50 and HETEROGENEITY . Completeness and contamination can be used for filtering (see params-file ). N50 , COVERAGE and HETEROGENEITY are used for selecting the representative of every cluster. You can set values of these columns to zero if data is not available or if you don't want the representative selection to be influenced by theses columns. Make sure that BIN_ID is a unique identifier. Output The output tsv file ( clusters.tsv in the cluster s folder) contains the columns CLUSTER , GENOME and REPRESENTATIVE where CLUSTER identifies a group of genomes, GENOME represents the path or link of a genome and REPRESENTATIVE is either 0 or 1 (selected as representative). If sans is specified in the configuration file (see examples folder), then [SANS](https://gitlab.ub.uni-bielefeld.de/gi/sans) is used to dereplicate the genomes of every cluster that was reported by the previous step. The SANS output can be found in the sans` folder.","title":"Dereplication"},{"location":"modules/dereplication/#dereplication","text":"","title":"Dereplication"},{"location":"modules/dereplication/#input","text":"Command Configuration File TSV Table -entry wDereplication -params-file example_params/dereplication.yml Must include the columns DATASET , BIN_ID , PATH , COMPLETENESS , CONTAMINATION , COVERAGE , N50 and HETEROGENEITY . Completeness and contamination can be used for filtering (see params-file ). N50 , COVERAGE and HETEROGENEITY are used for selecting the representative of every cluster. You can set values of these columns to zero if data is not available or if you don't want the representative selection to be influenced by theses columns. Make sure that BIN_ID is a unique identifier.","title":"Input"},{"location":"modules/dereplication/#output","text":"The output tsv file ( clusters.tsv in the cluster s folder) contains the columns CLUSTER , GENOME and REPRESENTATIVE where CLUSTER identifies a group of genomes, GENOME represents the path or link of a genome and REPRESENTATIVE is either 0 or 1 (selected as representative). If sans is specified in the configuration file (see examples folder), then [SANS](https://gitlab.ub.uni-bielefeld.de/gi/sans) is used to dereplicate the genomes of every cluster that was reported by the previous step. The SANS output can be found in the sans` folder.","title":"Output"},{"location":"modules/fragment_recruitment/","text":"Run Fragment Recruitment The fragment recruitment module can be used to find genomes in a set of read datasets. Note: This module only supports illumina data. Input Command Configuration file for fragment recruitment via mash screen and BWA Configuration file for fragment recruitment via mash screen and Bowtie Input TSV file for genomes Input TSV file for paired end reads Input TSV file for single end reads -entry wFragmentRecruitment -params-file example_params/fragmentRecruitment.yml NOTE! The file names of all provided genomes must be unique. The following parameters can be configured: mashDistCutoff: All hits below this threshold are discarded. mashHashCutoff: All hits that have a lower count of matched minimum hashes are discarded. coveredBasesCutoff: Number of bases that must be covered by at least one read. By how many reads the bases must be covered can be configured via the coverm setting (coverm: \" --min-covered-fraction 0 \"). Output The module outputs mash screen and bowtie alignment statistics. Furthermore, the module provides a coverm output which basically reports all metrics about the found genomes (e.g covered bases,length, tpm, ...).","title":"Run Fragment Recruitment"},{"location":"modules/fragment_recruitment/#run-fragment-recruitment","text":"The fragment recruitment module can be used to find genomes in a set of read datasets. Note: This module only supports illumina data.","title":"Run Fragment Recruitment"},{"location":"modules/fragment_recruitment/#input","text":"Command Configuration file for fragment recruitment via mash screen and BWA Configuration file for fragment recruitment via mash screen and Bowtie Input TSV file for genomes Input TSV file for paired end reads Input TSV file for single end reads -entry wFragmentRecruitment -params-file example_params/fragmentRecruitment.yml NOTE! The file names of all provided genomes must be unique. The following parameters can be configured: mashDistCutoff: All hits below this threshold are discarded. mashHashCutoff: All hits that have a lower count of matched minimum hashes are discarded. coveredBasesCutoff: Number of bases that must be covered by at least one read. By how many reads the bases must be covered can be configured via the coverm setting (coverm: \" --min-covered-fraction 0 \").","title":"Input"},{"location":"modules/fragment_recruitment/#output","text":"The module outputs mash screen and bowtie alignment statistics. Furthermore, the module provides a coverm output which basically reports all metrics about the found genomes (e.g covered bases,length, tpm, ...).","title":"Output"},{"location":"modules/magAttributes/","text":"MagAttributes Input Command Configuration File MAGs TSV Table -entry wMagAttributes -params-file example_params/magAttributes.yml Must include at least DATASET identifier and mag specific PATH and BIN_ID column. Databases Checkm and GTDB need their databases as input. See database section for possibly download strategies. The GTDB and Checkm compressed databases must be tar.gz files. If you provide the extracted version of GTDB using the extractedDBPath parameter, please specify the path to the releasesXXX directory (e.g. \"/vol/spool/gtdb/release202\"). If you need credentials to access your files via S3 then please use the following command: For GTDB: nextflow secrets set S3_gtdb_ACCESS XXXXXXX nextflow secrets set S3_gtdb_SECRET XXXXXXX For Checkm: nextflow secrets set S3_checkm_ACCESS XXXXXXX nextflow secrets set S3_checkm_SECRET XXXXXXX Output GTDBTk All GTDB files include the GTDB specific columns in addition to a SAMPLE column ( SAMPLE_gtdbtk.bac120.summary.tsv , SAMPLE_gtdbtk.ar122.summary.tsv ). In addition, this module produces a file SAMPLE_gtdbtk_CHUNK.tsv that combines both files and adds a BIN_ID column that adheres to the magAttributes specification Checkm and Checkm2 The Checkm and Checkm2 output adheres to the magAttributes specification and adds a BIN_ID and SAMPLE column to the output file. If Checkm2 and Checkm are both specified in the config file then only the Checkm2 results are used for downstream pipeline steps.","title":"MagAttributes"},{"location":"modules/magAttributes/#magattributes","text":"","title":"MagAttributes"},{"location":"modules/magAttributes/#input","text":"Command Configuration File MAGs TSV Table -entry wMagAttributes -params-file example_params/magAttributes.yml Must include at least DATASET identifier and mag specific PATH and BIN_ID column.","title":"Input"},{"location":"modules/magAttributes/#databases","text":"Checkm and GTDB need their databases as input. See database section for possibly download strategies. The GTDB and Checkm compressed databases must be tar.gz files. If you provide the extracted version of GTDB using the extractedDBPath parameter, please specify the path to the releasesXXX directory (e.g. \"/vol/spool/gtdb/release202\"). If you need credentials to access your files via S3 then please use the following command: For GTDB: nextflow secrets set S3_gtdb_ACCESS XXXXXXX nextflow secrets set S3_gtdb_SECRET XXXXXXX For Checkm: nextflow secrets set S3_checkm_ACCESS XXXXXXX nextflow secrets set S3_checkm_SECRET XXXXXXX","title":"Databases"},{"location":"modules/magAttributes/#output","text":"","title":"Output"},{"location":"modules/magAttributes/#gtdbtk","text":"All GTDB files include the GTDB specific columns in addition to a SAMPLE column ( SAMPLE_gtdbtk.bac120.summary.tsv , SAMPLE_gtdbtk.ar122.summary.tsv ). In addition, this module produces a file SAMPLE_gtdbtk_CHUNK.tsv that combines both files and adds a BIN_ID column that adheres to the magAttributes specification","title":"GTDBTk"},{"location":"modules/magAttributes/#checkm-and-checkm2","text":"The Checkm and Checkm2 output adheres to the magAttributes specification and adds a BIN_ID and SAMPLE column to the output file. If Checkm2 and Checkm are both specified in the config file then only the Checkm2 results are used for downstream pipeline steps.","title":"Checkm and Checkm2"},{"location":"modules/metabolomics/","text":"Metabolomics The metabolomics module runs genome scale metabolic modeling analysis based on a supplied genome or directly on its proteins. The module is able to use gapseq and carveme for analysing genomes and carveme for analysing predicted proteins which depends on the configuration you provide as input. Note: If carvem is specificed in fullPipeline mode then carveme is executed with proteins as input. All generated models are used for further downstream analysis such as the \"Minimum Resource Overlap\" computation by smetana. Input Command Configuration file for providing genomes -entry wMetabolomics -params-file example_params/metabolomics Almost all tools of this module are using linear programming solvers. The tool developers are recommending the use of the cplex solver that is included in the IBM ILOG CPLEX Optimization Studio which is free for students and academics through the IBM Academic Initiative programm. Since the toolkit uses docker images that are downloaded from public Docker Hub repositories and the cplex license is not allowed to be distributed, we prepared a Dockerfile that allows you to build your own local docker image with all metabolomics specific tools installed. Just copy your cplex binary to the cplex docker folder and build your own docker image. You can override all existing images via the command line. In the following example your the image name is metabolomics:0.1.0: --gapseq_image=metabolomics:0.1.0 (Optional) --smetana_image=metabolomics:0.1.0 (Required) --carveme_image=metabolomics:0.1.0 (Optional. Carveme is not able to detect the solver automatically. Please specify --solver in the configuration file if you want to use the scip solver.) --memote_image=metabolomics:0.1.0 (Optional. Memote is not able to detect the solver automatically. Please specify --solver in the configuration file if you are not using the glpk solver.) For gapseq and memote we are using a publicly available docker image that uses the freely available glkp solver which means that you don't have to provide this parameter. If you want to build your own image, please use the beforeProcessScript parameter. This parameter expects a bash script that accepts the docker image name as a parameter. The script is executed right before the actual docker image is started. You could for example provide a script that builds the actual image right before running the tool on the VM. It would be also possible to push the docker image to a private dockerhub repository and login to your docker account via this script. We provide two example template scripts in the cplex folder. Please note that in both cases you distribute the docker image with your cplex binary on all machines where you run the toolkit. If you login to dockerhub then your credentials will saved on the VM. If you are not the only docker user on the machine we do not recommend this approach! Output GapSeq / CarveMe Both tools are generating genome scale metabolic reconstruction models ( *.xml ). All models are translated to json format and substrats, products and reactions are saved in distinct files. Memote Memote tests metabolic reconstruction models and therefore produces a machine readable json file ( *_report.json.gz ) and a human readable tsv ( *_metrics.tsv ) and html ( *_report.html ) file. Smetana Smetana is used for analysing possible interactions in microbial communities. Smetana s global and detailed modes are executed per sample. The Smetana output is saved in _detailed.tsv and _global.tsv`.","title":"Metabolomics"},{"location":"modules/metabolomics/#metabolomics","text":"The metabolomics module runs genome scale metabolic modeling analysis based on a supplied genome or directly on its proteins. The module is able to use gapseq and carveme for analysing genomes and carveme for analysing predicted proteins which depends on the configuration you provide as input. Note: If carvem is specificed in fullPipeline mode then carveme is executed with proteins as input. All generated models are used for further downstream analysis such as the \"Minimum Resource Overlap\" computation by smetana.","title":"Metabolomics"},{"location":"modules/metabolomics/#input","text":"Command Configuration file for providing genomes -entry wMetabolomics -params-file example_params/metabolomics Almost all tools of this module are using linear programming solvers. The tool developers are recommending the use of the cplex solver that is included in the IBM ILOG CPLEX Optimization Studio which is free for students and academics through the IBM Academic Initiative programm. Since the toolkit uses docker images that are downloaded from public Docker Hub repositories and the cplex license is not allowed to be distributed, we prepared a Dockerfile that allows you to build your own local docker image with all metabolomics specific tools installed. Just copy your cplex binary to the cplex docker folder and build your own docker image. You can override all existing images via the command line. In the following example your the image name is metabolomics:0.1.0: --gapseq_image=metabolomics:0.1.0 (Optional) --smetana_image=metabolomics:0.1.0 (Required) --carveme_image=metabolomics:0.1.0 (Optional. Carveme is not able to detect the solver automatically. Please specify --solver in the configuration file if you want to use the scip solver.) --memote_image=metabolomics:0.1.0 (Optional. Memote is not able to detect the solver automatically. Please specify --solver in the configuration file if you are not using the glpk solver.) For gapseq and memote we are using a publicly available docker image that uses the freely available glkp solver which means that you don't have to provide this parameter. If you want to build your own image, please use the beforeProcessScript parameter. This parameter expects a bash script that accepts the docker image name as a parameter. The script is executed right before the actual docker image is started. You could for example provide a script that builds the actual image right before running the tool on the VM. It would be also possible to push the docker image to a private dockerhub repository and login to your docker account via this script. We provide two example template scripts in the cplex folder. Please note that in both cases you distribute the docker image with your cplex binary on all machines where you run the toolkit. If you login to dockerhub then your credentials will saved on the VM. If you are not the only docker user on the machine we do not recommend this approach!","title":"Input"},{"location":"modules/metabolomics/#output","text":"","title":"Output"},{"location":"modules/metabolomics/#gapseq-carveme","text":"Both tools are generating genome scale metabolic reconstruction models ( *.xml ). All models are translated to json format and substrats, products and reactions are saved in distinct files.","title":"GapSeq / CarveMe"},{"location":"modules/metabolomics/#memote","text":"Memote tests metabolic reconstruction models and therefore produces a machine readable json file ( *_report.json.gz ) and a human readable tsv ( *_metrics.tsv ) and html ( *_report.html ) file.","title":"Memote"},{"location":"modules/metabolomics/#smetana","text":"Smetana is used for analysing possible interactions in microbial communities. Smetana s global and detailed modes are executed per sample. The Smetana output is saved in _detailed.tsv and _global.tsv`.","title":"Smetana"},{"location":"modules/plasmids/","text":"Plasmids The plasmid module is able to identify contigs as plasmids and also to assemble plasmids from the samples fastq data. The module is executed in two parts. In the first part contigs of a metagenome assembler are scanned for plasmids. In the second part a plasmid assembler is used to assemble circular plasmids out of raw reads. All plasmid detection tools are executed on the circular assembly result and on the contigs of the metagenome assembler. Just the filtered sequences are used for downstream analysis. The identification of plasmids is based on the combined result of tools which have a filter property assigned. Results of all tools that have the filter property set to true are combined either by a logical OR or by a logical AND . Example for the OR and AND operations: Let's assume that we have three plasmid detection tools (t1, t2, t3) that have four contigs (c1, c2, c3, c4) as input. Let's further assume that c1 and c2 are detected by all tools as contigs and c3 and c4 are only detected by t1 and t2. By using an AND only c1 and c2 are finally reported by the module as plasmids. By using an OR all contigs would be annotated as plasmids. It is also possible to simply run a tool without using its result as filter by setting filter to false . If a tool should not be executed then the tool section should be removed. Only the detected plasmids will be used for downstream analysis. For running a plasmid assembly we suggest running the full pipeline mode with the enabled plasmids module. See input example configuration files. The read mapper can either be Bowtie or Bwa for Illumina and minimap for long reads. Input Command Configuration file for full pipeline mode with plasmids detections Configuration file for plasmids module only TSV Table -entry wPlasmidsPath -params-file example_params/plasmids.yml Databases The plasmid module needs the following compressed database file formats: ViralVerifyPlasmid ViralVerifyPlasmid needs a recent pfam-A database in .gz format. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_ViralVerifyPlasmid_ACCESS XXXXXXX nextflow secrets set S3_ViralVerifyPlasmid_SECRET XXXXXXX MobTyper Database was generated by gzipping the output of mob_init. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_MobTyper_ACCESS XXXXXXX nextflow secrets set S3_MobTyper_SECRET XXXXXXX Platon The tar gzipped database for running platon can be fetched from the Platon github page. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_Platon_ACCESS XXXXXXX nextflow secrets set S3_Platon_SECRET XXXXXXX PLSDB PLSDB Database is available via this link: https://ccb-microbe.cs.uni-saarland.de/plsdb/plasmids/download/plasmids_meta.tar.bz2 . All files except .tsv and .msh were deleted from the compressed package. See database section for possible download strategies. The compressed database must be a tar.bz2 file. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_PLSDB_ACCESS XXXXXXX nextflow secrets set S3_PLSDB_SECRET XXXXXXX Output SCAPP SCAPP detects plasmid sequences out of the samples assembly graph. It reports sequences as gzipped fasta files ( *_plasmids.fasta.gz ). A basic statistic ( *_plasmids_stats.tsv ) per plasmid and a summary statistic ( *_plasmids_summary_stats.tsv ) over all plasmids is also generated. Coverm coverage metrics are generated for all plasmids. Gene coverage values are generated as part of the annotation module output. PlasClass PlasClass is able to identify plasmids by using a statistical model that was build using kmer frequencies. It reports gzipped fata files and their probabilities ( *_plasclass.tsv ). MobTyper and Platon MobTyper and Platon are using both replicon typing for plasmid detection. ( *_mobtyper_results.tsv , *_platon.tsv ) ViralVerifyPlasmid ViralVerfiy is applying a Naive Bayes classifier ( *_viralverifyplasmid.tsv ). PLSDB PLSDB includes a curated set of plasmid sequences that were extracted from databases like refseq. The metadata of found sequences are reported in *.tsv and the metadata of the filtered sequences in *_kmerThreshold_X.tsv .","title":"Plasmids"},{"location":"modules/plasmids/#plasmids","text":"The plasmid module is able to identify contigs as plasmids and also to assemble plasmids from the samples fastq data. The module is executed in two parts. In the first part contigs of a metagenome assembler are scanned for plasmids. In the second part a plasmid assembler is used to assemble circular plasmids out of raw reads. All plasmid detection tools are executed on the circular assembly result and on the contigs of the metagenome assembler. Just the filtered sequences are used for downstream analysis. The identification of plasmids is based on the combined result of tools which have a filter property assigned. Results of all tools that have the filter property set to true are combined either by a logical OR or by a logical AND . Example for the OR and AND operations: Let's assume that we have three plasmid detection tools (t1, t2, t3) that have four contigs (c1, c2, c3, c4) as input. Let's further assume that c1 and c2 are detected by all tools as contigs and c3 and c4 are only detected by t1 and t2. By using an AND only c1 and c2 are finally reported by the module as plasmids. By using an OR all contigs would be annotated as plasmids. It is also possible to simply run a tool without using its result as filter by setting filter to false . If a tool should not be executed then the tool section should be removed. Only the detected plasmids will be used for downstream analysis. For running a plasmid assembly we suggest running the full pipeline mode with the enabled plasmids module. See input example configuration files. The read mapper can either be Bowtie or Bwa for Illumina and minimap for long reads.","title":"Plasmids"},{"location":"modules/plasmids/#input","text":"Command Configuration file for full pipeline mode with plasmids detections Configuration file for plasmids module only TSV Table -entry wPlasmidsPath -params-file example_params/plasmids.yml","title":"Input"},{"location":"modules/plasmids/#databases","text":"The plasmid module needs the following compressed database file formats:","title":"Databases"},{"location":"modules/plasmids/#viralverifyplasmid","text":"ViralVerifyPlasmid needs a recent pfam-A database in .gz format. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_ViralVerifyPlasmid_ACCESS XXXXXXX nextflow secrets set S3_ViralVerifyPlasmid_SECRET XXXXXXX","title":"ViralVerifyPlasmid"},{"location":"modules/plasmids/#mobtyper","text":"Database was generated by gzipping the output of mob_init. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_MobTyper_ACCESS XXXXXXX nextflow secrets set S3_MobTyper_SECRET XXXXXXX","title":"MobTyper"},{"location":"modules/plasmids/#platon","text":"The tar gzipped database for running platon can be fetched from the Platon github page. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_Platon_ACCESS XXXXXXX nextflow secrets set S3_Platon_SECRET XXXXXXX","title":"Platon"},{"location":"modules/plasmids/#plsdb","text":"PLSDB Database is available via this link: https://ccb-microbe.cs.uni-saarland.de/plsdb/plasmids/download/plasmids_meta.tar.bz2 . All files except .tsv and .msh were deleted from the compressed package. See database section for possible download strategies. The compressed database must be a tar.bz2 file. If you need credentials to access your files via S3 then please use the following command: nextflow secrets set S3_PLSDB_ACCESS XXXXXXX nextflow secrets set S3_PLSDB_SECRET XXXXXXX","title":"PLSDB"},{"location":"modules/plasmids/#output","text":"","title":"Output"},{"location":"modules/plasmids/#scapp","text":"SCAPP detects plasmid sequences out of the samples assembly graph. It reports sequences as gzipped fasta files ( *_plasmids.fasta.gz ). A basic statistic ( *_plasmids_stats.tsv ) per plasmid and a summary statistic ( *_plasmids_summary_stats.tsv ) over all plasmids is also generated. Coverm coverage metrics are generated for all plasmids. Gene coverage values are generated as part of the annotation module output.","title":"SCAPP"},{"location":"modules/plasmids/#plasclass","text":"PlasClass is able to identify plasmids by using a statistical model that was build using kmer frequencies. It reports gzipped fata files and their probabilities ( *_plasclass.tsv ).","title":"PlasClass"},{"location":"modules/plasmids/#mobtyper-and-platon","text":"MobTyper and Platon are using both replicon typing for plasmid detection. ( *_mobtyper_results.tsv , *_platon.tsv )","title":"MobTyper and Platon"},{"location":"modules/plasmids/#viralverifyplasmid_1","text":"ViralVerfiy is applying a Naive Bayes classifier ( *_viralverifyplasmid.tsv ).","title":"ViralVerifyPlasmid"},{"location":"modules/plasmids/#plsdb_1","text":"PLSDB includes a curated set of plasmid sequences that were extracted from databases like refseq. The metadata of found sequences are reported in *.tsv and the metadata of the filtered sequences in *_kmerThreshold_X.tsv .","title":"PLSDB"},{"location":"modules/qualityControl/","text":"Quality Control The quality control module removes adapters, trims and filters short read and long read data. Input Command for short read data Command for nanopore data TSV Table short read TSV Table nanopore -entry wShortReadQualityControl -params-file example_params/qc.yml -entry wOntQualityControl -params-file example_params/qcONT.yml Output The output is a gzipped fastq file (short read: SAMPLE_interleaved.qc.fq.gz , long read: SAMPLE_qc.fq.gz ) containing trimmed and quality filtered reads.","title":"Quality Control"},{"location":"modules/qualityControl/#quality-control","text":"The quality control module removes adapters, trims and filters short read and long read data.","title":"Quality Control"},{"location":"modules/qualityControl/#input","text":"Command for short read data Command for nanopore data TSV Table short read TSV Table nanopore -entry wShortReadQualityControl -params-file example_params/qc.yml -entry wOntQualityControl -params-file example_params/qcONT.yml","title":"Input"},{"location":"modules/qualityControl/#output","text":"The output is a gzipped fastq file (short read: SAMPLE_interleaved.qc.fq.gz , long read: SAMPLE_qc.fq.gz ) containing trimmed and quality filtered reads.","title":"Output"},{"location":"modules/readMapping/","text":"Read Mapping Note: This module only supports illumina data. Input Command Configuration File MAGs TSV Table Samples TSV Table -entry wReadMapping -params-file example_params/readMapping.yml Output The produced output files are the following: count.tsv, mean.tsv, mean_mincov10.tsv, rpkm.tsv, tpm.tsv, trimmed_mean.tsv. The content of the files are produced by coverm. All metrics are explained on the coverm GitHub page: https://github.com/wwood/CoverM .","title":"Read Mapping"},{"location":"modules/readMapping/#read-mapping","text":"Note: This module only supports illumina data.","title":"Read Mapping"},{"location":"modules/readMapping/#input","text":"Command Configuration File MAGs TSV Table Samples TSV Table -entry wReadMapping -params-file example_params/readMapping.yml","title":"Input"},{"location":"modules/readMapping/#output","text":"The produced output files are the following: count.tsv, mean.tsv, mean_mincov10.tsv, rpkm.tsv, tpm.tsv, trimmed_mean.tsv. The content of the files are produced by coverm. All metrics are explained on the coverm GitHub page: https://github.com/wwood/CoverM .","title":"Output"}]}